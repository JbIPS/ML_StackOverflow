{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ec12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aa357",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709851e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;I have a UIImageView that I have already se...</td>\n",
       "      <td>[iphone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;h2&gt;The requirements I'm up against&lt;/h2&gt;\\n\\n&lt;p...</td>\n",
       "      <td>[c#, winforms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;I am using the xml-simple gem inside a rake...</td>\n",
       "      <td>[ruby, xml]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;p&gt;I'm trying to get a PHP array that includes...</td>\n",
       "      <td>[php]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;p&gt;I have done some Google searching and found...</td>\n",
       "      <td>[python, sql, django]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99706</th>\n",
       "      <td>&lt;p&gt;So I'm trying to get rid of my std::vector'...</td>\n",
       "      <td>[c++]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99707</th>\n",
       "      <td>&lt;p&gt;Is it possible to have a singleton in a fac...</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99708</th>\n",
       "      <td>&lt;p&gt;I was wondering if there is some option to ...</td>\n",
       "      <td>[c#, visual-studio-2008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99709</th>\n",
       "      <td>&lt;p&gt;I have found the \"Getting Started\" document...</td>\n",
       "      <td>[iphone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99710</th>\n",
       "      <td>&lt;p&gt;I have a 2D area with \"dots\" distributed on...</td>\n",
       "      <td>[algorithm]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75481 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Body  \\\n",
       "0      <p>I have a UIImageView that I have already se...   \n",
       "2      <h2>The requirements I'm up against</h2>\\n\\n<p...   \n",
       "4      <p>I am using the xml-simple gem inside a rake...   \n",
       "5      <p>I'm trying to get a PHP array that includes...   \n",
       "7      <p>I have done some Google searching and found...   \n",
       "...                                                  ...   \n",
       "99706  <p>So I'm trying to get rid of my std::vector'...   \n",
       "99707  <p>Is it possible to have a singleton in a fac...   \n",
       "99708  <p>I was wondering if there is some option to ...   \n",
       "99709  <p>I have found the \"Getting Started\" document...   \n",
       "99710  <p>I have a 2D area with \"dots\" distributed on...   \n",
       "\n",
       "                           Tags  \n",
       "0                      [iphone]  \n",
       "2                [c#, winforms]  \n",
       "4                   [ruby, xml]  \n",
       "5                         [php]  \n",
       "7         [python, sql, django]  \n",
       "...                         ...  \n",
       "99706                     [c++]  \n",
       "99707                    [java]  \n",
       "99708  [c#, visual-studio-2008]  \n",
       "99709                  [iphone]  \n",
       "99710               [algorithm]  \n",
       "\n",
       "[75481 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b4148",
   "metadata": {},
   "source": [
    "# Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e443f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    def __init__(self, print_cm, print_report, feature_names, target_names = None):\n",
    "        self.print_cm = print_cm\n",
    "        self.print_report = print_report\n",
    "        self.target_names = target_names\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "def benchmark(data, clf, clf_name, opts):\n",
    "    (X_train, X_test, y_train, y_test) = data\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    f1_micro = metrics.f1_score(y_test, pred, average='micro')\n",
    "    print(\"f1-score (micro):   %0.3f\" % f1_micro)\n",
    "    f1_macro = metrics.f1_score(y_test, pred, average='macro')\n",
    "    print(\"f1-score (macro):   %0.3f\" % f1_macro)\n",
    "\n",
    "    coef = None\n",
    "    if hasattr(clf, 'estimators_') and all(hasattr(estimator, 'coef_') for estimator in clf.estimators_):\n",
    "        coef = np.concatenate([estimator.coef_ for estimator in clf.estimators_])\n",
    "    elif hasattr(clf, 'coef_'):\n",
    "        coef = clf.coef_\n",
    "        \n",
    "    if type(coef) != type(None):\n",
    "        print(\"dimensionality: %d\" % coef.shape[1])\n",
    "        print(\"density: %f\" % density(coef))\n",
    "\n",
    "        if opts.target_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(opts.target_names):\n",
    "                top10 = np.argsort(coef[i])[-10:]\n",
    "                print(\"%s: %s\" % (label, \" \".join(opts.target_names[top10])))\n",
    "        print()\n",
    "\n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=opts.target_names,\n",
    "                                            zero_division=0))\n",
    "\n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.multilabel_confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    return clf_name, score, f1_micro, f1_macro, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d781fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_pipeline(select_chi2 = None):\n",
    "    return Pipeline([\n",
    "        ('tf-idf', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('feature-reduction', SelectKBest(chi2, k=select_chi2) if select_chi2 else 'passthrough')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dee344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, y, select_chi2 = 500, print_cm = False, print_report = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    mlb = MultiLabelBinarizer().fit(y.to_list())\n",
    "    y_train = mlb.transform(y_train)\n",
    "    y_test = mlb.transform(y_test)\n",
    "    target_names = mlb.classes_\n",
    "\n",
    "    preprocess_pipeline = get_preprocessing_pipeline(select_chi2)\n",
    "    X_train = preprocess_pipeline.fit_transform(X_train, y_train)\n",
    "    X_test = preprocess_pipeline.transform(X_test)\n",
    "    \n",
    "    data = (X_train, X_test, y_train, y_test)\n",
    "    feature_names = preprocess_pipeline.named_steps['tf-idf'].get_feature_names()\n",
    "    opts = Opts(print_cm, print_report, target_names)\n",
    " \n",
    "    results = []\n",
    "    for clf, name in (\n",
    "            (RidgeClassifier(tol=1e-2, solver=\"sag\", random_state=1), \"Ridge Classifier\"),\n",
    "            (OneVsRestClassifier(Perceptron(max_iter=50, n_jobs=-1, random_state=2), n_jobs=-1), \"Perceptron\"),\n",
    "            (OneVsRestClassifier(PassiveAggressiveClassifier(max_iter=50, n_jobs=-1, random_state=3)),\n",
    "             \"Passive-Aggressive\"),\n",
    "            (KNeighborsClassifier(n_neighbors=10, n_jobs=-1), \"kNN\"),\n",
    "            (RandomForestClassifier(n_jobs=-1, random_state=5), \"Random forest\")):\n",
    "        print('=' * 80)\n",
    "        print(name)\n",
    "        results.append(benchmark(data, clf, name, opts))\n",
    "\n",
    "    for penalty in [\"l2\", \"l1\"]:\n",
    "        print('=' * 80)\n",
    "        print(\"%s penalty\" % penalty.upper())\n",
    "        # Train Liblinear model\n",
    "        results.append(benchmark(data, OneVsRestClassifier(LinearSVC(penalty=penalty, dual=False,\n",
    "                                           tol=1e-3, random_state=5)), f'LinearSVC {penalty.upper()}', opts))\n",
    "\n",
    "        # Train SGD model\n",
    "        results.append(benchmark(data, OneVsRestClassifier(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                               penalty=penalty, n_jobs=-1, random_state=3)), 'SGD Classifier', opts))\n",
    "\n",
    "    # Train SGD with Elastic Net penalty\n",
    "    print('=' * 80)\n",
    "    name = \"Elastic-Net penalty\"\n",
    "    print(name)\n",
    "    results.append(benchmark(data, OneVsRestClassifier(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                           penalty=\"elasticnet\", n_jobs=-1, random_state=5)), name, opts))\n",
    "\n",
    "    # Train sparse Naive Bayes classifiers\n",
    "    print('=' * 80)\n",
    "    print(\"Naive Bayes\")\n",
    "    results.append(benchmark(data, OneVsRestClassifier(MultinomialNB(alpha=.01)), 'Mutltinomial NB', opts))\n",
    "    results.append(benchmark(data, OneVsRestClassifier(BernoulliNB(alpha=.01)), 'Bernoulli NB', opts))\n",
    "    results.append(benchmark(data, OneVsRestClassifier(ComplementNB(alpha=.1)), 'Complement NB', opts))\n",
    "\n",
    "    indices = np.arange(len(results))\n",
    "\n",
    "    results = [[x[i] for x in results] for i in range(6)]\n",
    "\n",
    "    clf_names, score, f1_micro, f1_macro, training_time, test_time = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.title(\"Score\")\n",
    "    plt.barh(indices, score, .1, label=\"score\", color='navy')\n",
    "    plt.barh(indices + .2, f1_micro, .1, label=\"F1 (micro)\", color='b')\n",
    "    plt.barh(indices + .4, f1_macro, .1, label=\"F1 (macro)\", color='m')\n",
    "    plt.barh(indices + .6, training_time, .1, label=\"training time\",\n",
    "             color='c')\n",
    "    plt.barh(indices + .8, test_time, .1, label=\"test time\", color='darkorange')\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clf_names):\n",
    "        plt.text(-.3, i, c)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    result_df = pd.DataFrame(np.array(results[1:]).T, index=results[0], columns=['Score', 'F1 micro', 'F1 macro', 'Training time', 'Testing time'])\n",
    "    display(result_df)\n",
    "\n",
    "    print('Best score algorithm:')\n",
    "    print(f'{result_df[\"Score\"].idxmax()}: {result_df[\"Score\"].max()}')\n",
    "    print('_' * 80)\n",
    "    print('Best f1 micro algorithm:')\n",
    "    print(f'{result_df[\"F1 micro\"].idxmax()}: {result_df[\"F1 micro\"].max()}')\n",
    "    print('_' * 80)\n",
    "    print('Best f1 macro algorithm:')\n",
    "    print(f'{result_df[\"F1 macro\"].idxmax()}: {result_df[\"F1 macro\"].max()}')\n",
    "    print('_' * 80)\n",
    "    print('Fastest training algorithm:')\n",
    "    print(f'{result_df[\"Training time\"].idxmin()}: {result_df[\"Training time\"].min()}')\n",
    "    print('_' * 80)\n",
    "    print('Fastest testing algorithm:')\n",
    "    print(f'{result_df[\"Testing time\"].idxmin()}: {result_df[\"Testing time\"].min()}')\n",
    "    print('_' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d7a88",
   "metadata": {},
   "source": [
    "# Ã‰valuation des diffÃ©rents modÃ¨les\n",
    "\n",
    "Nous allons tester diffÃ©rentes combinaisons de modÃ¨les compatible avec des matrices creuses pour une prÃ©diction multilabels.\n",
    "Les modÃ¨les seront ensuite Ã©valuer sur leur F-score et leur temps d'entrainement et de prÃ©diction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab525db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:729: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(random_state=1, solver='sag', tol=0.01)\n",
      "train time: 2.447s\n",
      "test time:  0.004s\n",
      "accuracy:   0.255\n",
      "f1-score (micro):   0.493\n",
      "f1-score (micro):   0.455\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.16      0.25      1362\n",
      "           1       0.66      0.33      0.44       207\n",
      "           2       0.78      0.24      0.37       175\n",
      "           3       0.98      0.62      0.76       465\n",
      "           4       0.65      0.21      0.31       217\n",
      "           5       0.80      0.43      0.56       973\n",
      "           6       0.84      0.56      0.67       257\n",
      "           7       0.88      0.22      0.35       425\n",
      "           8       0.58      0.14      0.22      2210\n",
      "           9       0.84      0.30      0.44      1009\n",
      "          10       0.79      0.51      0.62       430\n",
      "          11       0.38      0.01      0.02       346\n",
      "          12       0.93      0.67      0.78       177\n",
      "          13       0.81      0.66      0.73       161\n",
      "          14       0.85      0.64      0.73       147\n",
      "          15       0.69      0.22      0.34       649\n",
      "          16       0.78      0.16      0.26       160\n",
      "          17       0.80      0.48      0.60       461\n",
      "          18       0.86      0.51      0.64      1543\n",
      "          19       0.79      0.48      0.60      1250\n",
      "          20       0.87      0.63      0.73       657\n",
      "          21       0.74      0.46      0.57       171\n",
      "          22       0.72      0.30      0.42       229\n",
      "          23       0.73      0.23      0.35       130\n",
      "          24       0.73      0.44      0.55       169\n",
      "          25       0.82      0.45      0.58       486\n",
      "          26       0.62      0.25      0.36       272\n",
      "          27       0.81      0.46      0.58       192\n",
      "          28       0.48      0.05      0.09       220\n",
      "          29       0.88      0.55      0.68      1100\n",
      "          30       0.89      0.56      0.69       935\n",
      "          31       0.84      0.52      0.64       236\n",
      "          32       0.85      0.39      0.54       298\n",
      "          33       0.87      0.59      0.71       297\n",
      "          34       0.55      0.07      0.12       176\n",
      "          35       0.56      0.16      0.25       665\n",
      "          36       0.69      0.32      0.44       519\n",
      "          37       0.00      0.00      0.00       178\n",
      "          38       0.91      0.83      0.87       168\n",
      "          39       0.77      0.46      0.57       173\n",
      "          40       0.40      0.02      0.03       118\n",
      "          41       0.70      0.41      0.52       240\n",
      "          42       0.58      0.18      0.27       279\n",
      "          43       0.68      0.12      0.20       176\n",
      "          44       0.75      0.29      0.42       187\n",
      "          45       0.76      0.11      0.20       143\n",
      "          46       0.66      0.07      0.12       404\n",
      "          47       0.76      0.19      0.30       256\n",
      "          48       0.89      0.62      0.73       286\n",
      "          49       0.69      0.42      0.52       284\n",
      "\n",
      "   micro avg       0.80      0.36      0.49     22268\n",
      "   macro avg       0.73      0.35      0.45     22268\n",
      "weighted avg       0.75      0.36      0.46     22268\n",
      " samples avg       0.43      0.38      0.39     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=Perceptron(max_iter=50, n_jobs=-1,\n",
      "                                         random_state=2),\n",
      "                    n_jobs=-1)\n",
      "train time: 0.763s\n",
      "test time:  0.011s\n",
      "accuracy:   0.226\n",
      "f1-score (micro):   0.487\n",
      "f1-score (micro):   0.477\n",
      "dimensionality: 500\n",
      "density: 0.825880\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.18      0.24      1362\n",
      "           1       0.53      0.57      0.55       207\n",
      "           2       0.45      0.49      0.47       175\n",
      "           3       0.66      0.80      0.72       465\n",
      "           4       0.42      0.50      0.46       217\n",
      "           5       0.59      0.50      0.54       973\n",
      "           6       0.74      0.73      0.73       257\n",
      "           7       0.55      0.28      0.37       425\n",
      "           8       0.39      0.27      0.32      2210\n",
      "           9       0.37      0.54      0.44      1009\n",
      "          10       0.67      0.57      0.62       430\n",
      "          11       0.27      0.04      0.07       346\n",
      "          12       0.74      0.82      0.78       177\n",
      "          13       0.72      0.70      0.71       161\n",
      "          14       0.45      0.78      0.57       147\n",
      "          15       0.35      0.49      0.41       649\n",
      "          16       0.34      0.28      0.30       160\n",
      "          17       0.75      0.52      0.61       461\n",
      "          18       0.72      0.53      0.61      1543\n",
      "          19       0.45      0.68      0.54      1250\n",
      "          20       0.64      0.58      0.61       657\n",
      "          21       0.53      0.52      0.53       171\n",
      "          22       0.22      0.43      0.29       229\n",
      "          23       0.32      0.37      0.34       130\n",
      "          24       0.54      0.66      0.60       169\n",
      "          25       0.41      0.72      0.52       486\n",
      "          26       0.41      0.27      0.32       272\n",
      "          27       0.63      0.57      0.60       192\n",
      "          28       0.06      0.05      0.05       220\n",
      "          29       0.60      0.70      0.65      1100\n",
      "          30       0.76      0.63      0.69       935\n",
      "          31       0.72      0.69      0.70       236\n",
      "          32       0.46      0.66      0.54       298\n",
      "          33       0.55      0.74      0.63       297\n",
      "          34       0.25      0.28      0.26       176\n",
      "          35       0.43      0.32      0.37       665\n",
      "          36       0.57      0.44      0.50       519\n",
      "          37       0.16      0.12      0.14       178\n",
      "          38       0.79      0.92      0.85       168\n",
      "          39       0.52      0.69      0.59       173\n",
      "          40       0.14      0.17      0.15       118\n",
      "          41       0.47      0.49      0.48       240\n",
      "          42       0.26      0.42      0.32       279\n",
      "          43       0.20      0.62      0.31       176\n",
      "          44       0.51      0.53      0.52       187\n",
      "          45       0.48      0.22      0.30       143\n",
      "          46       0.32      0.41      0.36       404\n",
      "          47       0.30      0.69      0.41       256\n",
      "          48       0.71      0.79      0.75       286\n",
      "          49       0.61      0.29      0.40       284\n",
      "\n",
      "   micro avg       0.48      0.49      0.49     22268\n",
      "   macro avg       0.48      0.50      0.48     22268\n",
      "weighted avg       0.50      0.49      0.48     22268\n",
      " samples avg       0.48      0.52      0.46     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=PassiveAggressiveClassifier(max_iter=50,\n",
      "                                                          n_jobs=-1,\n",
      "                                                          random_state=3))\n",
      "train time: 0.984s\n",
      "test time:  0.010s\n",
      "accuracy:   0.310\n",
      "f1-score (micro):   0.556\n",
      "f1-score (micro):   0.509\n",
      "dimensionality: 500\n",
      "density: 0.889520\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.22      0.32      1362\n",
      "           1       0.64      0.34      0.44       207\n",
      "           2       0.67      0.42      0.52       175\n",
      "           3       0.96      0.77      0.86       465\n",
      "           4       0.75      0.18      0.28       217\n",
      "           5       0.79      0.50      0.61       973\n",
      "           6       0.82      0.72      0.77       257\n",
      "           7       0.81      0.27      0.40       425\n",
      "           8       0.64      0.11      0.19      2210\n",
      "           9       0.87      0.35      0.50      1009\n",
      "          10       0.77      0.65      0.71       430\n",
      "          11       0.00      0.00      0.00       346\n",
      "          12       0.94      0.82      0.88       177\n",
      "          13       0.77      0.76      0.77       161\n",
      "          14       0.84      0.73      0.78       147\n",
      "          15       0.67      0.27      0.39       649\n",
      "          16       0.65      0.21      0.32       160\n",
      "          17       0.81      0.56      0.66       461\n",
      "          18       0.82      0.64      0.72      1543\n",
      "          19       0.79      0.57      0.66      1250\n",
      "          20       0.81      0.72      0.76       657\n",
      "          21       0.74      0.57      0.64       171\n",
      "          22       0.72      0.40      0.51       229\n",
      "          23       0.75      0.31      0.44       130\n",
      "          24       0.69      0.53      0.60       169\n",
      "          25       0.79      0.51      0.62       486\n",
      "          26       0.62      0.36      0.45       272\n",
      "          27       0.83      0.66      0.73       192\n",
      "          28       0.00      0.00      0.00       220\n",
      "          29       0.87      0.65      0.75      1100\n",
      "          30       0.88      0.69      0.77       935\n",
      "          31       0.79      0.69      0.73       236\n",
      "          32       0.81      0.49      0.61       298\n",
      "          33       0.85      0.72      0.78       297\n",
      "          34       0.60      0.03      0.06       176\n",
      "          35       0.61      0.14      0.23       665\n",
      "          36       0.69      0.44      0.54       519\n",
      "          37       0.00      0.00      0.00       178\n",
      "          38       0.86      0.89      0.87       168\n",
      "          39       0.74      0.60      0.66       173\n",
      "          40       0.00      0.00      0.00       118\n",
      "          41       0.65      0.57      0.61       240\n",
      "          42       0.61      0.26      0.36       279\n",
      "          43       0.58      0.12      0.21       176\n",
      "          44       0.64      0.47      0.54       187\n",
      "          45       0.75      0.15      0.25       143\n",
      "          46       1.00      0.01      0.02       404\n",
      "          47       0.70      0.29      0.40       256\n",
      "          48       0.89      0.78      0.83       286\n",
      "          49       0.64      0.69      0.66       284\n",
      "\n",
      "   micro avg       0.78      0.43      0.56     22268\n",
      "   macro avg       0.70      0.44      0.51     22268\n",
      "weighted avg       0.73      0.43      0.51     22268\n",
      " samples avg       0.52      0.46      0.47     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(n_jobs=-1, n_neighbors=10)\n",
      "train time: 0.051s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  20.373s\n",
      "accuracy:   0.253\n",
      "f1-score (micro):   0.448\n",
      "f1-score (micro):   0.423\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.13      0.21      1362\n",
      "           1       0.77      0.17      0.28       207\n",
      "           2       0.69      0.34      0.45       175\n",
      "           3       0.98      0.56      0.72       465\n",
      "           4       0.65      0.18      0.28       217\n",
      "           5       0.82      0.33      0.47       973\n",
      "           6       0.92      0.41      0.57       257\n",
      "           7       0.78      0.25      0.38       425\n",
      "           8       0.54      0.19      0.28      2210\n",
      "           9       0.83      0.32      0.46      1009\n",
      "          10       0.85      0.40      0.54       430\n",
      "          11       0.53      0.05      0.10       346\n",
      "          12       0.97      0.49      0.65       177\n",
      "          13       0.86      0.57      0.69       161\n",
      "          14       0.84      0.52      0.64       147\n",
      "          15       0.66      0.14      0.23       649\n",
      "          16       0.67      0.11      0.19       160\n",
      "          17       0.81      0.48      0.60       461\n",
      "          18       0.87      0.44      0.58      1543\n",
      "          19       0.78      0.38      0.51      1250\n",
      "          20       0.84      0.39      0.53       657\n",
      "          21       0.77      0.28      0.41       171\n",
      "          22       0.74      0.33      0.45       229\n",
      "          23       0.73      0.21      0.32       130\n",
      "          24       0.67      0.38      0.48       169\n",
      "          25       0.78      0.35      0.48       486\n",
      "          26       0.67      0.21      0.31       272\n",
      "          27       0.87      0.39      0.53       192\n",
      "          28       0.47      0.04      0.07       220\n",
      "          29       0.89      0.45      0.60      1100\n",
      "          30       0.88      0.51      0.65       935\n",
      "          31       0.85      0.51      0.64       236\n",
      "          32       0.81      0.41      0.54       298\n",
      "          33       0.85      0.45      0.59       297\n",
      "          34       0.57      0.10      0.17       176\n",
      "          35       0.55      0.20      0.30       665\n",
      "          36       0.69      0.33      0.45       519\n",
      "          37       0.42      0.04      0.08       178\n",
      "          38       0.91      0.82      0.86       168\n",
      "          39       0.73      0.40      0.52       173\n",
      "          40       0.42      0.04      0.08       118\n",
      "          41       0.68      0.34      0.45       240\n",
      "          42       0.60      0.26      0.36       279\n",
      "          43       0.60      0.14      0.23       176\n",
      "          44       0.72      0.34      0.46       187\n",
      "          45       0.71      0.07      0.13       143\n",
      "          46       0.60      0.14      0.22       404\n",
      "          47       0.78      0.15      0.25       256\n",
      "          48       0.91      0.41      0.57       286\n",
      "          49       0.74      0.43      0.54       284\n",
      "\n",
      "   micro avg       0.77      0.32      0.45     22268\n",
      "   macro avg       0.74      0.31      0.42     22268\n",
      "weighted avg       0.74      0.32      0.43     22268\n",
      " samples avg       0.43      0.35      0.37     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(n_jobs=-1, random_state=5)\n",
      "train time: 16.197s\n",
      "test time:  1.100s\n",
      "accuracy:   0.302\n",
      "f1-score (micro):   0.512\n",
      "f1-score (micro):   0.440\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.21      0.31      1362\n",
      "           1       0.62      0.16      0.25       207\n",
      "           2       0.64      0.29      0.40       175\n",
      "           3       0.92      0.71      0.80       465\n",
      "           4       0.66      0.17      0.27       217\n",
      "           5       0.79      0.47      0.59       973\n",
      "           6       0.86      0.46      0.60       257\n",
      "           7       0.61      0.24      0.34       425\n",
      "           8       0.56      0.23      0.33      2210\n",
      "           9       0.70      0.34      0.46      1009\n",
      "          10       0.84      0.49      0.62       430\n",
      "          11       0.50      0.05      0.09       346\n",
      "          12       0.94      0.62      0.75       177\n",
      "          13       0.79      0.60      0.68       161\n",
      "          14       0.88      0.47      0.61       147\n",
      "          15       0.63      0.19      0.30       649\n",
      "          16       0.50      0.05      0.09       160\n",
      "          17       0.76      0.54      0.63       461\n",
      "          18       0.79      0.58      0.67      1543\n",
      "          19       0.79      0.57      0.66      1250\n",
      "          20       0.83      0.65      0.73       657\n",
      "          21       0.76      0.37      0.50       171\n",
      "          22       0.64      0.28      0.39       229\n",
      "          23       0.68      0.16      0.26       130\n",
      "          24       0.59      0.26      0.36       169\n",
      "          25       0.82      0.50      0.62       486\n",
      "          26       0.53      0.17      0.25       272\n",
      "          27       0.84      0.46      0.60       192\n",
      "          28       0.45      0.04      0.07       220\n",
      "          29       0.83      0.62      0.71      1100\n",
      "          30       0.84      0.67      0.74       935\n",
      "          31       0.80      0.45      0.58       236\n",
      "          32       0.77      0.41      0.54       298\n",
      "          33       0.88      0.61      0.72       297\n",
      "          34       0.20      0.03      0.06       176\n",
      "          35       0.66      0.29      0.40       665\n",
      "          36       0.75      0.44      0.55       519\n",
      "          37       0.40      0.09      0.15       178\n",
      "          38       0.89      0.73      0.80       168\n",
      "          39       0.71      0.24      0.36       173\n",
      "          40       0.40      0.03      0.06       118\n",
      "          41       0.58      0.28      0.38       240\n",
      "          42       0.55      0.25      0.34       279\n",
      "          43       0.58      0.12      0.21       176\n",
      "          44       0.68      0.12      0.21       187\n",
      "          45       0.40      0.10      0.16       143\n",
      "          46       0.51      0.18      0.26       404\n",
      "          47       0.72      0.20      0.32       256\n",
      "          48       0.90      0.63      0.74       286\n",
      "          49       0.77      0.36      0.49       284\n",
      "\n",
      "   micro avg       0.75      0.39      0.51     22268\n",
      "   macro avg       0.69      0.34      0.44     22268\n",
      "weighted avg       0.70      0.39      0.49     22268\n",
      " samples avg       0.51      0.43      0.45     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=LinearSVC(dual=False, random_state=5, tol=0.001))\n",
      "train time: 2.185s\n",
      "test time:  0.010s\n",
      "accuracy:   0.295\n",
      "f1-score (micro):   0.543\n",
      "f1-score (micro):   0.504\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.19      0.29      1362\n",
      "           1       0.64      0.37      0.47       207\n",
      "           2       0.76      0.31      0.44       175\n",
      "           3       0.96      0.73      0.83       465\n",
      "           4       0.64      0.26      0.37       217\n",
      "           5       0.78      0.47      0.59       973\n",
      "           6       0.83      0.66      0.73       257\n",
      "           7       0.83      0.26      0.40       425\n",
      "           8       0.60      0.18      0.28      2210\n",
      "           9       0.83      0.35      0.49      1009\n",
      "          10       0.81      0.58      0.68       430\n",
      "          11       0.29      0.01      0.02       346\n",
      "          12       0.94      0.76      0.84       177\n",
      "          13       0.80      0.68      0.73       161\n",
      "          14       0.86      0.69      0.76       147\n",
      "          15       0.64      0.27      0.38       649\n",
      "          16       0.81      0.21      0.34       160\n",
      "          17       0.81      0.53      0.64       461\n",
      "          18       0.85      0.58      0.69      1543\n",
      "          19       0.79      0.54      0.64      1250\n",
      "          20       0.86      0.68      0.76       657\n",
      "          21       0.75      0.54      0.63       171\n",
      "          22       0.73      0.34      0.46       229\n",
      "          23       0.75      0.30      0.43       130\n",
      "          24       0.71      0.47      0.56       169\n",
      "          25       0.80      0.51      0.63       486\n",
      "          26       0.64      0.28      0.39       272\n",
      "          27       0.83      0.52      0.64       192\n",
      "          28       0.45      0.05      0.08       220\n",
      "          29       0.87      0.63      0.73      1100\n",
      "          30       0.90      0.65      0.75       935\n",
      "          31       0.79      0.61      0.69       236\n",
      "          32       0.83      0.45      0.58       298\n",
      "          33       0.87      0.69      0.77       297\n",
      "          34       0.61      0.08      0.14       176\n",
      "          35       0.61      0.25      0.36       665\n",
      "          36       0.69      0.40      0.50       519\n",
      "          37       0.58      0.04      0.07       178\n",
      "          38       0.88      0.88      0.88       168\n",
      "          39       0.77      0.52      0.62       173\n",
      "          40       0.44      0.03      0.06       118\n",
      "          41       0.70      0.45      0.55       240\n",
      "          42       0.61      0.22      0.32       279\n",
      "          43       0.67      0.16      0.26       176\n",
      "          44       0.75      0.38      0.50       187\n",
      "          45       0.81      0.17      0.29       143\n",
      "          46       0.65      0.13      0.22       404\n",
      "          47       0.71      0.25      0.37       256\n",
      "          48       0.88      0.69      0.77       286\n",
      "          49       0.70      0.52      0.60       284\n",
      "\n",
      "   micro avg       0.79      0.41      0.54     22268\n",
      "   macro avg       0.74      0.41      0.50     22268\n",
      "weighted avg       0.75      0.41      0.52     22268\n",
      " samples avg       0.50      0.44      0.45     22268\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=SGDClassifier(max_iter=50, n_jobs=-1,\n",
      "                                            random_state=3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.943s\n",
      "test time:  0.009s\n",
      "accuracy:   0.210\n",
      "f1-score (micro):   0.405\n",
      "f1-score (micro):   0.307\n",
      "dimensionality: 500\n",
      "density: 0.830400\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1362\n",
      "           1       0.00      0.00      0.00       207\n",
      "           2       0.00      0.00      0.00       175\n",
      "           3       0.97      0.61      0.75       465\n",
      "           4       0.00      0.00      0.00       217\n",
      "           5       0.78      0.40      0.53       973\n",
      "           6       0.83      0.47      0.60       257\n",
      "           7       0.91      0.10      0.17       425\n",
      "           8       0.00      0.00      0.00      2210\n",
      "           9       0.93      0.22      0.36      1009\n",
      "          10       0.83      0.46      0.59       430\n",
      "          11       0.00      0.00      0.00       346\n",
      "          12       0.93      0.64      0.76       177\n",
      "          13       0.80      0.65      0.72       161\n",
      "          14       0.81      0.50      0.62       147\n",
      "          15       1.00      0.00      0.01       649\n",
      "          16       0.00      0.00      0.00       160\n",
      "          17       0.84      0.37      0.52       461\n",
      "          18       0.82      0.54      0.65      1543\n",
      "          19       0.81      0.45      0.58      1250\n",
      "          20       0.86      0.58      0.69       657\n",
      "          21       0.85      0.20      0.33       171\n",
      "          22       1.00      0.01      0.02       229\n",
      "          23       0.00      0.00      0.00       130\n",
      "          24       0.80      0.12      0.21       169\n",
      "          25       0.76      0.40      0.53       486\n",
      "          26       0.89      0.03      0.06       272\n",
      "          27       0.80      0.41      0.54       192\n",
      "          28       0.00      0.00      0.00       220\n",
      "          29       0.84      0.59      0.69      1100\n",
      "          30       0.91      0.55      0.69       935\n",
      "          31       0.84      0.42      0.56       236\n",
      "          32       0.86      0.36      0.51       298\n",
      "          33       0.89      0.57      0.69       297\n",
      "          34       0.00      0.00      0.00       176\n",
      "          35       0.00      0.00      0.00       665\n",
      "          36       0.78      0.16      0.27       519\n",
      "          37       0.00      0.00      0.00       178\n",
      "          38       0.89      0.84      0.87       168\n",
      "          39       0.82      0.29      0.43       173\n",
      "          40       0.00      0.00      0.00       118\n",
      "          41       0.78      0.22      0.34       240\n",
      "          42       0.00      0.00      0.00       279\n",
      "          43       0.00      0.00      0.00       176\n",
      "          44       1.00      0.02      0.04       187\n",
      "          45       0.00      0.00      0.00       143\n",
      "          46       0.00      0.00      0.00       404\n",
      "          47       1.00      0.01      0.02       256\n",
      "          48       0.90      0.56      0.69       286\n",
      "          49       0.84      0.18      0.30       284\n",
      "\n",
      "   micro avg       0.85      0.27      0.40     22268\n",
      "   macro avg       0.59      0.24      0.31     22268\n",
      "weighted avg       0.65      0.27      0.34     22268\n",
      " samples avg       0.35      0.29      0.31     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=LinearSVC(dual=False, penalty='l1',\n",
      "                                        random_state=5, tol=0.001))\n",
      "train time: 5.049s\n",
      "test time:  0.010s\n",
      "accuracy:   0.299\n",
      "f1-score (micro):   0.548\n",
      "f1-score (micro):   0.509\n",
      "dimensionality: 500\n",
      "density: 0.407440\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.19      0.30      1362\n",
      "           1       0.65      0.39      0.48       207\n",
      "           2       0.73      0.31      0.44       175\n",
      "           3       0.96      0.75      0.84       465\n",
      "           4       0.65      0.24      0.35       217\n",
      "           5       0.78      0.48      0.59       973\n",
      "           6       0.84      0.67      0.74       257\n",
      "           7       0.82      0.27      0.40       425\n",
      "           8       0.61      0.19      0.29      2210\n",
      "           9       0.83      0.36      0.50      1009\n",
      "          10       0.80      0.59      0.68       430\n",
      "          11       0.31      0.01      0.02       346\n",
      "          12       0.95      0.78      0.85       177\n",
      "          13       0.78      0.68      0.72       161\n",
      "          14       0.83      0.69      0.75       147\n",
      "          15       0.64      0.27      0.38       649\n",
      "          16       0.69      0.21      0.33       160\n",
      "          17       0.81      0.54      0.64       461\n",
      "          18       0.85      0.58      0.69      1543\n",
      "          19       0.79      0.54      0.64      1250\n",
      "          20       0.86      0.68      0.76       657\n",
      "          21       0.74      0.54      0.63       171\n",
      "          22       0.73      0.34      0.46       229\n",
      "          23       0.75      0.32      0.44       130\n",
      "          24       0.72      0.49      0.58       169\n",
      "          25       0.79      0.52      0.63       486\n",
      "          26       0.62      0.30      0.40       272\n",
      "          27       0.80      0.56      0.66       192\n",
      "          28       0.50      0.05      0.10       220\n",
      "          29       0.87      0.64      0.74      1100\n",
      "          30       0.90      0.66      0.76       935\n",
      "          31       0.78      0.61      0.69       236\n",
      "          32       0.83      0.46      0.59       298\n",
      "          33       0.86      0.70      0.78       297\n",
      "          34       0.62      0.09      0.15       176\n",
      "          35       0.60      0.26      0.36       665\n",
      "          36       0.69      0.41      0.51       519\n",
      "          37       0.70      0.04      0.07       178\n",
      "          38       0.85      0.88      0.86       168\n",
      "          39       0.76      0.53      0.62       173\n",
      "          40       0.43      0.03      0.05       118\n",
      "          41       0.70      0.47      0.56       240\n",
      "          42       0.60      0.22      0.32       279\n",
      "          43       0.66      0.15      0.25       176\n",
      "          44       0.74      0.40      0.52       187\n",
      "          45       0.80      0.17      0.28       143\n",
      "          46       0.67      0.14      0.23       404\n",
      "          47       0.71      0.26      0.38       256\n",
      "          48       0.88      0.70      0.78       286\n",
      "          49       0.70      0.52      0.60       284\n",
      "\n",
      "   micro avg       0.78      0.42      0.55     22268\n",
      "   macro avg       0.74      0.42      0.51     22268\n",
      "weighted avg       0.75      0.42      0.52     22268\n",
      " samples avg       0.50      0.45      0.46     22268\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=SGDClassifier(max_iter=50, n_jobs=-1,\n",
      "                                            penalty='l1', random_state=3))\n",
      "train time: 1.205s\n",
      "test time:  0.009s\n",
      "accuracy:   0.250\n",
      "f1-score (micro):   0.475\n",
      "f1-score (micro):   0.386\n",
      "dimensionality: 500\n",
      "density: 0.025560\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.08      0.15      1362\n",
      "           1       0.79      0.13      0.22       207\n",
      "           2       0.75      0.17      0.28       175\n",
      "           3       0.95      0.70      0.81       465\n",
      "           4       0.00      0.00      0.00       217\n",
      "           5       0.77      0.48      0.59       973\n",
      "           6       0.76      0.59      0.66       257\n",
      "           7       0.81      0.20      0.33       425\n",
      "           8       0.53      0.02      0.04      2210\n",
      "           9       0.89      0.30      0.45      1009\n",
      "          10       0.77      0.53      0.63       430\n",
      "          11       0.00      0.00      0.00       346\n",
      "          12       0.90      0.71      0.79       177\n",
      "          13       0.75      0.79      0.77       161\n",
      "          14       0.76      0.64      0.69       147\n",
      "          15       1.00      0.02      0.05       649\n",
      "          16       0.00      0.00      0.00       160\n",
      "          17       0.81      0.49      0.61       461\n",
      "          18       0.80      0.62      0.69      1543\n",
      "          19       0.78      0.52      0.62      1250\n",
      "          20       0.79      0.61      0.69       657\n",
      "          21       0.65      0.52      0.58       171\n",
      "          22       0.91      0.17      0.29       229\n",
      "          23       0.00      0.00      0.00       130\n",
      "          24       0.73      0.33      0.45       169\n",
      "          25       0.73      0.52      0.61       486\n",
      "          26       0.76      0.14      0.24       272\n",
      "          27       0.74      0.61      0.67       192\n",
      "          28       0.00      0.00      0.00       220\n",
      "          29       0.80      0.65      0.72      1100\n",
      "          30       0.86      0.66      0.75       935\n",
      "          31       0.74      0.52      0.61       236\n",
      "          32       0.78      0.48      0.59       298\n",
      "          33       0.83      0.64      0.72       297\n",
      "          34       0.00      0.00      0.00       176\n",
      "          35       0.00      0.00      0.00       665\n",
      "          36       0.70      0.23      0.34       519\n",
      "          37       0.00      0.00      0.00       178\n",
      "          38       0.77      0.87      0.82       168\n",
      "          39       0.76      0.38      0.51       173\n",
      "          40       0.00      0.00      0.00       118\n",
      "          41       0.64      0.52      0.57       240\n",
      "          42       0.00      0.00      0.00       279\n",
      "          43       0.00      0.00      0.00       176\n",
      "          44       0.71      0.16      0.26       187\n",
      "          45       0.00      0.00      0.00       143\n",
      "          46       0.00      0.00      0.00       404\n",
      "          47       0.74      0.18      0.28       256\n",
      "          48       0.86      0.71      0.78       286\n",
      "          49       0.72      0.31      0.43       284\n",
      "\n",
      "   micro avg       0.79      0.34      0.48     22268\n",
      "   macro avg       0.58      0.32      0.39     22268\n",
      "weighted avg       0.66      0.34      0.41     22268\n",
      " samples avg       0.43      0.37      0.38     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=SGDClassifier(max_iter=50, n_jobs=-1,\n",
      "                                            penalty='elasticnet',\n",
      "                                            random_state=5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.627s\n",
      "test time:  0.009s\n",
      "accuracy:   0.214\n",
      "f1-score (micro):   0.411\n",
      "f1-score (micro):   0.312\n",
      "dimensionality: 500\n",
      "density: 0.175040\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1362\n",
      "           1       0.00      0.00      0.00       207\n",
      "           2       0.00      0.00      0.00       175\n",
      "           3       0.97      0.62      0.76       465\n",
      "           4       0.00      0.00      0.00       217\n",
      "           5       0.78      0.41      0.53       973\n",
      "           6       0.82      0.47      0.60       257\n",
      "           7       0.89      0.10      0.18       425\n",
      "           8       1.00      0.00      0.00      2210\n",
      "           9       0.92      0.22      0.36      1009\n",
      "          10       0.81      0.48      0.60       430\n",
      "          11       0.00      0.00      0.00       346\n",
      "          12       0.93      0.66      0.77       177\n",
      "          13       0.78      0.66      0.71       161\n",
      "          14       0.81      0.54      0.64       147\n",
      "          15       1.00      0.00      0.01       649\n",
      "          16       0.00      0.00      0.00       160\n",
      "          17       0.85      0.38      0.53       461\n",
      "          18       0.82      0.55      0.66      1543\n",
      "          19       0.81      0.45      0.58      1250\n",
      "          20       0.85      0.57      0.69       657\n",
      "          21       0.85      0.24      0.37       171\n",
      "          22       1.00      0.00      0.01       229\n",
      "          23       0.00      0.00      0.00       130\n",
      "          24       0.77      0.14      0.23       169\n",
      "          25       0.75      0.42      0.54       486\n",
      "          26       0.92      0.04      0.08       272\n",
      "          27       0.79      0.45      0.57       192\n",
      "          28       0.00      0.00      0.00       220\n",
      "          29       0.84      0.60      0.70      1100\n",
      "          30       0.91      0.57      0.70       935\n",
      "          31       0.82      0.42      0.56       236\n",
      "          32       0.83      0.38      0.52       298\n",
      "          33       0.88      0.56      0.69       297\n",
      "          34       0.00      0.00      0.00       176\n",
      "          35       0.00      0.00      0.00       665\n",
      "          36       0.79      0.17      0.28       519\n",
      "          37       0.00      0.00      0.00       178\n",
      "          38       0.88      0.83      0.85       168\n",
      "          39       0.82      0.32      0.46       173\n",
      "          40       0.00      0.00      0.00       118\n",
      "          41       0.78      0.25      0.38       240\n",
      "          42       0.00      0.00      0.00       279\n",
      "          43       0.00      0.00      0.00       176\n",
      "          44       1.00      0.01      0.02       187\n",
      "          45       0.00      0.00      0.00       143\n",
      "          46       0.00      0.00      0.00       404\n",
      "          47       1.00      0.01      0.02       256\n",
      "          48       0.90      0.57      0.70       286\n",
      "          49       0.84      0.18      0.30       284\n",
      "\n",
      "   micro avg       0.84      0.27      0.41     22268\n",
      "   macro avg       0.61      0.25      0.31     22268\n",
      "weighted avg       0.74      0.27      0.34     22268\n",
      " samples avg       0.36      0.30      0.31     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01))\n",
      "train time: 0.292s\n",
      "test time:  0.075s\n",
      "accuracy:   0.033\n",
      "f1-score (micro):   0.080\n",
      "f1-score (micro):   0.078\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1362\n",
      "           1       0.00      0.00      0.00       207\n",
      "           2       1.00      0.01      0.01       175\n",
      "           3       0.98      0.20      0.33       465\n",
      "           4       0.00      0.00      0.00       217\n",
      "           5       0.88      0.01      0.01       973\n",
      "           6       1.00      0.05      0.09       257\n",
      "           7       1.00      0.04      0.08       425\n",
      "           8       0.36      0.00      0.00      2210\n",
      "           9       0.91      0.12      0.21      1009\n",
      "          10       0.88      0.19      0.31       430\n",
      "          11       0.00      0.00      0.00       346\n",
      "          12       1.00      0.20      0.33       177\n",
      "          13       1.00      0.01      0.01       161\n",
      "          14       0.88      0.16      0.27       147\n",
      "          15       0.75      0.10      0.18       649\n",
      "          16       0.39      0.04      0.08       160\n",
      "          17       0.74      0.11      0.19       461\n",
      "          18       0.83      0.03      0.06      1543\n",
      "          19       0.74      0.06      0.11      1250\n",
      "          20       0.87      0.04      0.08       657\n",
      "          21       0.00      0.00      0.00       171\n",
      "          22       1.00      0.00      0.01       229\n",
      "          23       0.00      0.00      0.00       130\n",
      "          24       0.00      0.00      0.00       169\n",
      "          25       0.90      0.02      0.04       486\n",
      "          26       0.51      0.07      0.12       272\n",
      "          27       0.70      0.04      0.07       192\n",
      "          28       0.00      0.00      0.00       220\n",
      "          29       0.95      0.02      0.04      1100\n",
      "          30       0.84      0.05      0.09       935\n",
      "          31       0.88      0.03      0.06       236\n",
      "          32       0.73      0.04      0.07       298\n",
      "          33       0.89      0.11      0.19       297\n",
      "          34       0.00      0.00      0.00       176\n",
      "          35       0.36      0.01      0.01       665\n",
      "          36       0.91      0.02      0.04       519\n",
      "          37       0.00      0.00      0.00       178\n",
      "          38       0.95      0.34      0.50       168\n",
      "          39       0.80      0.02      0.04       173\n",
      "          40       0.00      0.00      0.00       118\n",
      "          41       0.00      0.00      0.00       240\n",
      "          42       0.00      0.00      0.00       279\n",
      "          43       0.00      0.00      0.00       176\n",
      "          44       1.00      0.01      0.02       187\n",
      "          45       0.00      0.00      0.00       143\n",
      "          46       0.00      0.00      0.00       404\n",
      "          47       1.00      0.00      0.01       256\n",
      "          48       0.91      0.15      0.25       286\n",
      "          49       0.50      0.00      0.01       284\n",
      "\n",
      "   micro avg       0.83      0.04      0.08     22268\n",
      "   macro avg       0.56      0.05      0.08     22268\n",
      "weighted avg       0.62      0.04      0.07     22268\n",
      " samples avg       0.05      0.05      0.05     22268\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=BernoulliNB(alpha=0.01))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.361s\n",
      "test time:  0.096s\n",
      "accuracy:   0.159\n",
      "f1-score (micro):   0.495\n",
      "f1-score (micro):   0.488\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.49      0.39      1362\n",
      "           1       0.14      0.59      0.23       207\n",
      "           2       0.30      0.52      0.38       175\n",
      "           3       0.84      0.74      0.78       465\n",
      "           4       0.28      0.68      0.40       217\n",
      "           5       0.54      0.66      0.59       973\n",
      "           6       0.41      0.76      0.53       257\n",
      "           7       0.31      0.55      0.39       425\n",
      "           8       0.43      0.49      0.46      2210\n",
      "           9       0.45      0.59      0.51      1009\n",
      "          10       0.36      0.65      0.47       430\n",
      "          11       0.16      0.54      0.25       346\n",
      "          12       0.58      0.71      0.64       177\n",
      "          13       0.46      0.80      0.58       161\n",
      "          14       0.64      0.73      0.68       147\n",
      "          15       0.29      0.51      0.37       649\n",
      "          16       0.29      0.63      0.40       160\n",
      "          17       0.65      0.73      0.69       461\n",
      "          18       0.66      0.68      0.67      1543\n",
      "          19       0.50      0.62      0.55      1250\n",
      "          20       0.39      0.69      0.50       657\n",
      "          21       0.28      0.73      0.41       171\n",
      "          22       0.35      0.58      0.43       229\n",
      "          23       0.26      0.65      0.37       130\n",
      "          24       0.40      0.68      0.51       169\n",
      "          25       0.35      0.77      0.49       486\n",
      "          26       0.46      0.66      0.54       272\n",
      "          27       0.20      0.76      0.32       192\n",
      "          28       0.23      0.37      0.28       220\n",
      "          29       0.63      0.67      0.65      1100\n",
      "          30       0.80      0.73      0.76       935\n",
      "          31       0.65      0.73      0.69       236\n",
      "          32       0.63      0.71      0.67       298\n",
      "          33       0.69      0.75      0.72       297\n",
      "          34       0.22      0.41      0.29       176\n",
      "          35       0.39      0.79      0.52       665\n",
      "          36       0.36      0.80      0.49       519\n",
      "          37       0.13      0.39      0.20       178\n",
      "          38       0.66      0.96      0.78       168\n",
      "          39       0.52      0.76      0.62       173\n",
      "          40       0.13      0.30      0.18       118\n",
      "          41       0.41      0.66      0.50       240\n",
      "          42       0.26      0.63      0.37       279\n",
      "          43       0.20      0.68      0.31       176\n",
      "          44       0.36      0.69      0.47       187\n",
      "          45       0.22      0.59      0.32       143\n",
      "          46       0.27      0.57      0.36       404\n",
      "          47       0.36      0.66      0.46       256\n",
      "          48       0.66      0.72      0.69       286\n",
      "          49       0.49      0.63      0.55       284\n",
      "\n",
      "   micro avg       0.41      0.63      0.50     22268\n",
      "   macro avg       0.41      0.65      0.49     22268\n",
      "weighted avg       0.46      0.63      0.52     22268\n",
      " samples avg       0.41      0.64      0.46     22268\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=ComplementNB(alpha=0.1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.292s\n",
      "test time:  0.078s\n",
      "accuracy:   0.007\n",
      "f1-score (micro):   0.281\n",
      "f1-score (micro):   0.270\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.77      0.35      1362\n",
      "           1       0.07      0.93      0.13       207\n",
      "           2       0.06      0.79      0.11       175\n",
      "           3       0.28      0.92      0.43       465\n",
      "           4       0.08      0.93      0.16       217\n",
      "           5       0.26      0.86      0.40       973\n",
      "           6       0.12      0.90      0.21       257\n",
      "           7       0.14      0.76      0.23       425\n",
      "           8       0.33      0.79      0.47      2210\n",
      "           9       0.26      0.77      0.39      1009\n",
      "          10       0.23      0.93      0.37       430\n",
      "          11       0.11      0.83      0.20       346\n",
      "          12       0.16      0.89      0.28       177\n",
      "          13       0.09      0.90      0.17       161\n",
      "          14       0.20      0.97      0.33       147\n",
      "          15       0.20      0.84      0.33       649\n",
      "          16       0.13      0.89      0.23       160\n",
      "          17       0.39      0.89      0.54       461\n",
      "          18       0.37      0.83      0.51      1543\n",
      "          19       0.35      0.90      0.51      1250\n",
      "          20       0.22      0.94      0.36       657\n",
      "          21       0.07      0.93      0.13       171\n",
      "          22       0.07      0.78      0.13       229\n",
      "          23       0.05      0.87      0.10       130\n",
      "          24       0.08      0.96      0.15       169\n",
      "          25       0.20      0.91      0.32       486\n",
      "          26       0.23      0.88      0.37       272\n",
      "          27       0.10      0.91      0.17       192\n",
      "          28       0.04      0.70      0.07       220\n",
      "          29       0.28      0.89      0.43      1100\n",
      "          30       0.39      0.87      0.54       935\n",
      "          31       0.17      0.90      0.28       236\n",
      "          32       0.22      0.91      0.35       298\n",
      "          33       0.23      0.91      0.37       297\n",
      "          34       0.04      0.81      0.08       176\n",
      "          35       0.28      0.90      0.43       665\n",
      "          36       0.23      0.93      0.36       519\n",
      "          37       0.05      0.84      0.10       178\n",
      "          38       0.21      0.98      0.34       168\n",
      "          39       0.16      0.97      0.28       173\n",
      "          40       0.03      0.80      0.06       118\n",
      "          41       0.07      0.86      0.13       240\n",
      "          42       0.10      0.81      0.18       279\n",
      "          43       0.06      0.81      0.12       176\n",
      "          44       0.10      0.91      0.18       187\n",
      "          45       0.06      0.87      0.11       143\n",
      "          46       0.11      0.82      0.19       404\n",
      "          47       0.11      0.93      0.20       256\n",
      "          48       0.21      0.92      0.35       286\n",
      "          49       0.17      0.90      0.28       284\n",
      "\n",
      "   micro avg       0.17      0.86      0.28     22268\n",
      "   macro avg       0.17      0.87      0.27     22268\n",
      "weighted avg       0.24      0.86      0.36     22268\n",
      " samples avg       0.20      0.86      0.30     22268\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(steps=[('feature_selection',\n",
      "                 SelectFromModel(estimator=OneVsRestClassifier(estimator=LinearSVC(dual=False,\n",
      "                                                                                   penalty='l1',\n",
      "                                                                                   random_state=4,\n",
      "                                                                                   tol=0.001)))),\n",
      "                ('classification',\n",
      "                 OneVsRestClassifier(estimator=LinearSVC(random_state=8)))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). If you observe this warning while using RFE or SelectFromModel, use the importance_getter parameter instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 7.157s\n",
      "test time:  0.010s\n",
      "accuracy:   0.271\n",
      "f1-score (micro):   0.511\n",
      "f1-score (micro):   0.459\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.19      0.29      1362\n",
      "           1       0.63      0.34      0.44       207\n",
      "           2       0.65      0.21      0.32       175\n",
      "           3       0.95      0.68      0.79       465\n",
      "           4       0.61      0.19      0.29       217\n",
      "           5       0.78      0.45      0.57       973\n",
      "           6       0.82      0.55      0.66       257\n",
      "           7       0.83      0.22      0.35       425\n",
      "           8       0.58      0.16      0.25      2210\n",
      "           9       0.85      0.35      0.49      1009\n",
      "          10       0.75      0.42      0.54       430\n",
      "          11       0.29      0.01      0.02       346\n",
      "          12       0.93      0.63      0.75       177\n",
      "          13       0.78      0.66      0.72       161\n",
      "          14       0.85      0.56      0.68       147\n",
      "          15       0.60      0.19      0.29       649\n",
      "          16       0.71      0.15      0.25       160\n",
      "          17       0.80      0.48      0.60       461\n",
      "          18       0.85      0.57      0.68      1543\n",
      "          19       0.78      0.53      0.63      1250\n",
      "          20       0.85      0.61      0.71       657\n",
      "          21       0.75      0.49      0.59       171\n",
      "          22       0.70      0.31      0.43       229\n",
      "          23       0.67      0.24      0.35       130\n",
      "          24       0.70      0.36      0.47       169\n",
      "          25       0.79      0.48      0.60       486\n",
      "          26       0.68      0.28      0.39       272\n",
      "          27       0.80      0.51      0.62       192\n",
      "          28       0.50      0.05      0.10       220\n",
      "          29       0.87      0.62      0.73      1100\n",
      "          30       0.90      0.64      0.75       935\n",
      "          31       0.81      0.55      0.66       236\n",
      "          32       0.84      0.44      0.58       298\n",
      "          33       0.86      0.63      0.73       297\n",
      "          34       0.38      0.02      0.03       176\n",
      "          35       0.59      0.22      0.32       665\n",
      "          36       0.69      0.32      0.44       519\n",
      "          37       0.80      0.02      0.04       178\n",
      "          38       0.86      0.86      0.86       168\n",
      "          39       0.73      0.36      0.48       173\n",
      "          40       0.33      0.02      0.03       118\n",
      "          41       0.69      0.45      0.54       240\n",
      "          42       0.57      0.20      0.30       279\n",
      "          43       0.60      0.09      0.15       176\n",
      "          44       0.68      0.25      0.37       187\n",
      "          45       0.83      0.13      0.23       143\n",
      "          46       0.65      0.12      0.20       404\n",
      "          47       0.69      0.23      0.35       256\n",
      "          48       0.89      0.63      0.74       286\n",
      "          49       0.68      0.44      0.53       284\n",
      "\n",
      "   micro avg       0.78      0.38      0.51     22268\n",
      "   macro avg       0.72      0.36      0.46     22268\n",
      "weighted avg       0.74      0.38      0.48     22268\n",
      " samples avg       0.46      0.41      0.41     22268\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). If you observe this warning while using RFE or SelectFromModel, use the importance_getter parameter instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>F1 micro</th>\n",
       "      <th>F1 macro</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Testing time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>0.255216</td>\n",
       "      <td>0.492810</td>\n",
       "      <td>0.454668</td>\n",
       "      <td>2.447045</td>\n",
       "      <td>0.003553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.486681</td>\n",
       "      <td>0.476669</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.011053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive-Aggressive</th>\n",
       "      <td>0.310194</td>\n",
       "      <td>0.556067</td>\n",
       "      <td>0.508745</td>\n",
       "      <td>0.983844</td>\n",
       "      <td>0.009657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kNN</th>\n",
       "      <td>0.253030</td>\n",
       "      <td>0.448408</td>\n",
       "      <td>0.422558</td>\n",
       "      <td>0.051243</td>\n",
       "      <td>20.372884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest</th>\n",
       "      <td>0.302444</td>\n",
       "      <td>0.512391</td>\n",
       "      <td>0.440183</td>\n",
       "      <td>16.196765</td>\n",
       "      <td>1.100172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC L2</th>\n",
       "      <td>0.295026</td>\n",
       "      <td>0.543017</td>\n",
       "      <td>0.504459</td>\n",
       "      <td>2.185166</td>\n",
       "      <td>0.009595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD Classifier</th>\n",
       "      <td>0.210373</td>\n",
       "      <td>0.404648</td>\n",
       "      <td>0.306601</td>\n",
       "      <td>0.942787</td>\n",
       "      <td>0.009204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC L1</th>\n",
       "      <td>0.299199</td>\n",
       "      <td>0.548284</td>\n",
       "      <td>0.508541</td>\n",
       "      <td>5.049499</td>\n",
       "      <td>0.009871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD Classifier</th>\n",
       "      <td>0.249917</td>\n",
       "      <td>0.475472</td>\n",
       "      <td>0.385886</td>\n",
       "      <td>1.204906</td>\n",
       "      <td>0.009406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elastic-Net penalty</th>\n",
       "      <td>0.214480</td>\n",
       "      <td>0.410725</td>\n",
       "      <td>0.312227</td>\n",
       "      <td>1.627044</td>\n",
       "      <td>0.009236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mutltinomial NB</th>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.080431</td>\n",
       "      <td>0.078451</td>\n",
       "      <td>0.291720</td>\n",
       "      <td>0.074603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB</th>\n",
       "      <td>0.158972</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.488260</td>\n",
       "      <td>0.360970</td>\n",
       "      <td>0.095853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complement NB</th>\n",
       "      <td>0.007352</td>\n",
       "      <td>0.281260</td>\n",
       "      <td>0.269715</td>\n",
       "      <td>0.291869</td>\n",
       "      <td>0.078112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC L1 feature select</th>\n",
       "      <td>0.271113</td>\n",
       "      <td>0.511334</td>\n",
       "      <td>0.458538</td>\n",
       "      <td>7.157053</td>\n",
       "      <td>0.009593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Score  F1 micro  F1 macro  Training time  \\\n",
       "Ridge Classifier       0.255216  0.492810  0.454668       2.447045   \n",
       "Perceptron             0.225806  0.486681  0.476669       0.762712   \n",
       "Passive-Aggressive     0.310194  0.556067  0.508745       0.983844   \n",
       "kNN                    0.253030  0.448408  0.422558       0.051243   \n",
       "Random forest          0.302444  0.512391  0.440183      16.196765   \n",
       "LinearSVC L2           0.295026  0.543017  0.504459       2.185166   \n",
       "SGD Classifier         0.210373  0.404648  0.306601       0.942787   \n",
       "LinearSVC L1           0.299199  0.548284  0.508541       5.049499   \n",
       "SGD Classifier         0.249917  0.475472  0.385886       1.204906   \n",
       "Elastic-Net penalty    0.214480  0.410725  0.312227       1.627044   \n",
       "Mutltinomial NB        0.033450  0.080431  0.078451       0.291720   \n",
       "Bernoulli NB           0.158972  0.495008  0.488260       0.360970   \n",
       "Complement NB          0.007352  0.281260  0.269715       0.291869   \n",
       "SVC L1 feature select  0.271113  0.511334  0.458538       7.157053   \n",
       "\n",
       "                       Testing time  \n",
       "Ridge Classifier           0.003553  \n",
       "Perceptron                 0.011053  \n",
       "Passive-Aggressive         0.009657  \n",
       "kNN                       20.372884  \n",
       "Random forest              1.100172  \n",
       "LinearSVC L2               0.009595  \n",
       "SGD Classifier             0.009204  \n",
       "LinearSVC L1               0.009871  \n",
       "SGD Classifier             0.009406  \n",
       "Elastic-Net penalty        0.009236  \n",
       "Mutltinomial NB            0.074603  \n",
       "Bernoulli NB               0.095853  \n",
       "Complement NB              0.078112  \n",
       "SVC L1 feature select      0.009593  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score algorithm:\n",
      "Passive-Aggressive: 0.3101940782937007\n",
      "________________________________________________________________________________\n",
      "Best f1 micro algorithm:\n",
      "Passive-Aggressive: 0.5560673213406333\n",
      "________________________________________________________________________________\n",
      "Best f1 macro algorithm:\n",
      "Passive-Aggressive: 0.5087450961758788\n",
      "________________________________________________________________________________\n",
      "Fastest training algorithm:\n",
      "kNN: 0.051242828369140625\n",
      "________________________________________________________________________________\n",
      "Fastest testing algorithm:\n",
      "Ridge Classifier: 0.003552675247192383\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(dataset['Body'], dataset['Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccc591",
   "metadata": {},
   "source": [
    "Le modÃ¨le Passive-Aggressive sort en tÃªte, suivi de prÃ¨s par le LinearSVC L1. Cependant, un tuning des hyperparamÃ¨tres de ce dernier devrait pouvoir le faire passer au-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342787b",
   "metadata": {},
   "source": [
    "## Ã‰valuation avec un input lemmatisÃ©\n",
    "\n",
    "Regardons l'impact d'un preprocessing permettant de ne rÃ©cupÃ©rer que les lemmes de l'input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bdbf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [p, i, have, a, uiimageview, that, i, have, al...\n",
       "2        [h, the, requirements, i, m, up, against, h, p...\n",
       "4        [p, i, am, using, the, xml, simple, gem, insid...\n",
       "5        [p, i, m, trying, to, get, a, php, array, that...\n",
       "7        [p, i, have, done, some, google, searching, an...\n",
       "                               ...                        \n",
       "99706    [p, so, i, m, trying, to, get, rid, of, my, st...\n",
       "99707    [p, is, it, possible, to, have, a, singleton, ...\n",
       "99708    [p, i, was, wondering, if, there, is, some, op...\n",
       "99709    [p, i, have, found, the, getting, started, doc...\n",
       "99710    [p, i, have, a, d, area, with, dots, distribut...\n",
       "Name: Body, Length: 75481, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "tokens = dataset['Body'].str.lower().map(tokenizer.tokenize)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d58a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [p, i, have, a, uiimageview, that, i, have, al...\n",
       "2        [h, the, requirement, i, m, up, against, h, p,...\n",
       "4        [p, i, am, using, the, xml, simple, gem, insid...\n",
       "5        [p, i, m, trying, to, get, a, php, array, that...\n",
       "7        [p, i, have, done, some, google, searching, an...\n",
       "                               ...                        \n",
       "99706    [p, so, i, m, trying, to, get, rid, of, my, st...\n",
       "99707    [p, is, it, possible, to, have, a, singleton, ...\n",
       "99708    [p, i, wa, wondering, if, there, is, some, opt...\n",
       "99709    [p, i, have, found, the, getting, started, doc...\n",
       "99710    [p, i, have, a, d, area, with, dot, distribute...\n",
       "Name: Body, Length: 75481, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lem_tokens = tokens.map(lambda x: [wnl.lemmatize(w) for w in x])\n",
    "lem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61c6a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:729: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(random_state=1, solver='sag', tol=0.01)\n",
      "train time: 2.887s\n",
      "test time:  0.003s\n",
      "accuracy:   0.256\n",
      "f1-score (micro):   0.489\n",
      "f1-score (micro):   0.435\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.15      0.25      1358\n",
      "           1       0.62      0.38      0.47       196\n",
      "           2       0.66      0.19      0.29       178\n",
      "           3       0.97      0.57      0.72       507\n",
      "           4       0.60      0.20      0.30       188\n",
      "           5       0.78      0.39      0.52       928\n",
      "           6       0.80      0.52      0.63       260\n",
      "           7       0.77      0.21      0.33       412\n",
      "           8       0.63      0.14      0.23      2185\n",
      "           9       0.87      0.31      0.46       989\n",
      "          10       0.76      0.54      0.63       417\n",
      "          11       0.17      0.00      0.01       296\n",
      "          12       0.93      0.69      0.79       174\n",
      "          13       0.77      0.68      0.72       154\n",
      "          14       0.85      0.58      0.69       128\n",
      "          15       0.64      0.21      0.32       652\n",
      "          16       0.76      0.08      0.15       158\n",
      "          17       0.80      0.50      0.61       469\n",
      "          18       0.86      0.52      0.65      1583\n",
      "          19       0.77      0.45      0.57      1266\n",
      "          20       0.84      0.61      0.71       650\n",
      "          21       0.66      0.40      0.50       168\n",
      "          22       0.71      0.28      0.40       213\n",
      "          23       0.62      0.18      0.27       137\n",
      "          24       0.72      0.44      0.54       185\n",
      "          25       0.83      0.49      0.61       482\n",
      "          26       0.57      0.25      0.34       257\n",
      "          27       0.82      0.44      0.57       188\n",
      "          28       0.61      0.05      0.09       222\n",
      "          29       0.86      0.56      0.68      1121\n",
      "          30       0.89      0.58      0.70       939\n",
      "          31       0.86      0.51      0.64       235\n",
      "          32       0.73      0.38      0.50       300\n",
      "          33       0.88      0.57      0.69       347\n",
      "          34       0.56      0.07      0.12       149\n",
      "          35       0.65      0.17      0.27       632\n",
      "          36       0.72      0.34      0.46       493\n",
      "          37       0.00      0.00      0.00       175\n",
      "          38       0.93      0.74      0.83       191\n",
      "          39       0.83      0.44      0.58       162\n",
      "          40       0.75      0.02      0.05       129\n",
      "          41       0.68      0.37      0.48       220\n",
      "          42       0.54      0.18      0.27       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.68      0.31      0.43       181\n",
      "          45       0.47      0.05      0.09       136\n",
      "          46       0.64      0.02      0.05       383\n",
      "          47       0.76      0.17      0.27       234\n",
      "          48       0.93      0.62      0.75       295\n",
      "          49       0.70      0.42      0.52       285\n",
      "\n",
      "   micro avg       0.79      0.35      0.49     22081\n",
      "   macro avg       0.70      0.34      0.44     22081\n",
      "weighted avg       0.74      0.35      0.46     22081\n",
      " samples avg       0.43      0.38      0.39     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=Perceptron(max_iter=50, n_jobs=-1,\n",
      "                                         random_state=2),\n",
      "                    n_jobs=-1)\n",
      "train time: 0.339s\n",
      "test time:  0.025s\n",
      "accuracy:   0.217\n",
      "f1-score (micro):   0.486\n",
      "f1-score (micro):   0.466\n",
      "dimensionality: 500\n",
      "density: 0.828920\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.34      0.35      1358\n",
      "           1       0.33      0.44      0.38       196\n",
      "           2       0.50      0.39      0.44       178\n",
      "           3       0.88      0.74      0.80       507\n",
      "           4       0.33      0.48      0.39       188\n",
      "           5       0.47      0.53      0.50       928\n",
      "           6       0.64      0.60      0.62       260\n",
      "           7       0.36      0.38      0.37       412\n",
      "           8       0.38      0.39      0.39      2185\n",
      "           9       0.64      0.36      0.46       989\n",
      "          10       0.57      0.58      0.57       417\n",
      "          11       0.11      0.12      0.11       296\n",
      "          12       0.76      0.74      0.75       174\n",
      "          13       0.72      0.68      0.70       154\n",
      "          14       0.67      0.77      0.72       128\n",
      "          15       0.31      0.62      0.42       652\n",
      "          16       0.33      0.13      0.19       158\n",
      "          17       0.47      0.61      0.53       469\n",
      "          18       0.85      0.48      0.61      1583\n",
      "          19       0.53      0.65      0.59      1266\n",
      "          20       0.55      0.77      0.64       650\n",
      "          21       0.65      0.49      0.56       168\n",
      "          22       0.26      0.38      0.31       213\n",
      "          23       0.33      0.28      0.30       137\n",
      "          24       0.57      0.46      0.51       185\n",
      "          25       0.61      0.61      0.61       482\n",
      "          26       0.34      0.23      0.27       257\n",
      "          27       0.68      0.68      0.68       188\n",
      "          28       0.18      0.28      0.22       222\n",
      "          29       0.61      0.71      0.66      1121\n",
      "          30       0.77      0.68      0.72       939\n",
      "          31       0.72      0.47      0.57       235\n",
      "          32       0.55      0.43      0.48       300\n",
      "          33       0.57      0.69      0.63       347\n",
      "          34       0.15      0.31      0.20       149\n",
      "          35       0.49      0.28      0.36       632\n",
      "          36       0.45      0.41      0.43       493\n",
      "          37       0.08      0.46      0.14       175\n",
      "          38       0.82      0.82      0.82       191\n",
      "          39       0.60      0.59      0.59       162\n",
      "          40       0.19      0.14      0.16       129\n",
      "          41       0.59      0.47      0.52       220\n",
      "          42       0.34      0.40      0.37       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.37      0.54      0.44       181\n",
      "          45       0.25      0.25      0.25       136\n",
      "          46       0.30      0.25      0.28       383\n",
      "          47       0.46      0.41      0.44       234\n",
      "          48       0.91      0.60      0.73       295\n",
      "          49       0.57      0.49      0.53       285\n",
      "\n",
      "   micro avg       0.48      0.49      0.49     22081\n",
      "   macro avg       0.48      0.47      0.47     22081\n",
      "weighted avg       0.52      0.49      0.49     22081\n",
      " samples avg       0.47      0.52      0.46     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=PassiveAggressiveClassifier(max_iter=50,\n",
      "                                                          n_jobs=-1,\n",
      "                                                          random_state=3))\n",
      "train time: 1.034s\n",
      "test time:  0.010s\n",
      "accuracy:   0.321\n",
      "f1-score (micro):   0.561\n",
      "f1-score (micro):   0.494\n",
      "dimensionality: 500\n",
      "density: 0.895680\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.21      0.32      1358\n",
      "           1       0.62      0.33      0.43       196\n",
      "           2       0.78      0.36      0.49       178\n",
      "           3       0.96      0.73      0.83       507\n",
      "           4       0.60      0.23      0.34       188\n",
      "           5       0.74      0.52      0.61       928\n",
      "           6       0.80      0.64      0.71       260\n",
      "           7       0.72      0.24      0.36       412\n",
      "           8       0.66      0.19      0.30      2185\n",
      "           9       0.87      0.35      0.50       989\n",
      "          10       0.77      0.54      0.64       417\n",
      "          11       0.00      0.00      0.00       296\n",
      "          12       0.90      0.81      0.85       174\n",
      "          13       0.73      0.73      0.73       154\n",
      "          14       0.81      0.77      0.79       128\n",
      "          15       0.63      0.11      0.19       652\n",
      "          16       0.86      0.12      0.21       158\n",
      "          17       0.79      0.62      0.70       469\n",
      "          18       0.84      0.64      0.73      1583\n",
      "          19       0.76      0.57      0.65      1266\n",
      "          20       0.83      0.69      0.75       650\n",
      "          21       0.68      0.60      0.64       168\n",
      "          22       0.68      0.38      0.49       213\n",
      "          23       0.72      0.28      0.40       137\n",
      "          24       0.73      0.50      0.59       185\n",
      "          25       0.80      0.59      0.68       482\n",
      "          26       0.56      0.31      0.40       257\n",
      "          27       0.83      0.62      0.71       188\n",
      "          28       0.00      0.00      0.00       222\n",
      "          29       0.86      0.67      0.75      1121\n",
      "          30       0.86      0.70      0.77       939\n",
      "          31       0.82      0.70      0.75       235\n",
      "          32       0.70      0.46      0.56       300\n",
      "          33       0.86      0.71      0.78       347\n",
      "          34       1.00      0.01      0.01       149\n",
      "          35       0.63      0.28      0.39       632\n",
      "          36       0.69      0.49      0.57       493\n",
      "          37       0.00      0.00      0.00       175\n",
      "          38       0.93      0.86      0.89       191\n",
      "          39       0.79      0.62      0.69       162\n",
      "          40       0.00      0.00      0.00       129\n",
      "          41       0.65      0.51      0.57       220\n",
      "          42       0.63      0.18      0.28       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.66      0.43      0.52       181\n",
      "          45       0.64      0.05      0.10       136\n",
      "          46       1.00      0.01      0.01       383\n",
      "          47       0.70      0.39      0.50       234\n",
      "          48       0.89      0.76      0.82       295\n",
      "          49       0.72      0.67      0.69       285\n",
      "\n",
      "   micro avg       0.78      0.44      0.56     22081\n",
      "   macro avg       0.69      0.42      0.49     22081\n",
      "weighted avg       0.73      0.44      0.52     22081\n",
      " samples avg       0.53      0.47      0.48     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(n_jobs=-1, n_neighbors=10)\n",
      "train time: 0.051s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test time:  21.194s\n",
      "accuracy:   0.249\n",
      "f1-score (micro):   0.444\n",
      "f1-score (micro):   0.410\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.15      0.24      1358\n",
      "           1       0.64      0.14      0.23       196\n",
      "           2       0.72      0.33      0.45       178\n",
      "           3       0.96      0.54      0.69       507\n",
      "           4       0.52      0.21      0.30       188\n",
      "           5       0.83      0.31      0.46       928\n",
      "           6       0.88      0.41      0.56       260\n",
      "           7       0.71      0.21      0.33       412\n",
      "           8       0.57      0.22      0.32      2185\n",
      "           9       0.84      0.31      0.45       989\n",
      "          10       0.86      0.40      0.54       417\n",
      "          11       0.53      0.06      0.11       296\n",
      "          12       0.98      0.51      0.67       174\n",
      "          13       0.84      0.49      0.62       154\n",
      "          14       0.88      0.61      0.72       128\n",
      "          15       0.61      0.13      0.21       652\n",
      "          16       0.76      0.08      0.15       158\n",
      "          17       0.78      0.42      0.55       469\n",
      "          18       0.88      0.44      0.59      1583\n",
      "          19       0.77      0.36      0.49      1266\n",
      "          20       0.87      0.36      0.50       650\n",
      "          21       0.73      0.24      0.36       168\n",
      "          22       0.67      0.27      0.38       213\n",
      "          23       0.77      0.22      0.34       137\n",
      "          24       0.74      0.39      0.52       185\n",
      "          25       0.82      0.36      0.50       482\n",
      "          26       0.59      0.21      0.31       257\n",
      "          27       0.88      0.32      0.47       188\n",
      "          28       0.38      0.04      0.07       222\n",
      "          29       0.92      0.41      0.57      1121\n",
      "          30       0.89      0.47      0.62       939\n",
      "          31       0.80      0.48      0.60       235\n",
      "          32       0.76      0.35      0.48       300\n",
      "          33       0.92      0.41      0.57       347\n",
      "          34       0.61      0.07      0.13       149\n",
      "          35       0.56      0.27      0.37       632\n",
      "          36       0.73      0.39      0.51       493\n",
      "          37       0.44      0.05      0.08       175\n",
      "          38       0.94      0.69      0.79       191\n",
      "          39       0.93      0.46      0.61       162\n",
      "          40       0.59      0.08      0.14       129\n",
      "          41       0.64      0.31      0.42       220\n",
      "          42       0.54      0.23      0.32       289\n",
      "          43       0.43      0.02      0.03       185\n",
      "          44       0.72      0.36      0.48       181\n",
      "          45       0.47      0.07      0.12       136\n",
      "          46       0.49      0.08      0.14       383\n",
      "          47       0.59      0.14      0.22       234\n",
      "          48       0.94      0.46      0.62       295\n",
      "          49       0.76      0.41      0.53       285\n",
      "\n",
      "   micro avg       0.77      0.31      0.44     22081\n",
      "   macro avg       0.73      0.30      0.41     22081\n",
      "weighted avg       0.74      0.31      0.43     22081\n",
      " samples avg       0.42      0.34      0.37     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(n_jobs=-1, random_state=5)\n",
      "train time: 16.669s\n",
      "test time:  1.061s\n",
      "accuracy:   0.312\n",
      "f1-score (micro):   0.515\n",
      "f1-score (micro):   0.431\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.21      0.31      1358\n",
      "           1       0.51      0.09      0.16       196\n",
      "           2       0.77      0.26      0.39       178\n",
      "           3       0.95      0.67      0.79       507\n",
      "           4       0.58      0.19      0.28       188\n",
      "           5       0.79      0.45      0.58       928\n",
      "           6       0.84      0.37      0.52       260\n",
      "           7       0.64      0.21      0.32       412\n",
      "           8       0.58      0.24      0.34      2185\n",
      "           9       0.78      0.34      0.48       989\n",
      "          10       0.78      0.50      0.61       417\n",
      "          11       0.50      0.05      0.09       296\n",
      "          12       0.91      0.62      0.74       174\n",
      "          13       0.77      0.61      0.68       154\n",
      "          14       0.91      0.45      0.60       128\n",
      "          15       0.64      0.18      0.28       652\n",
      "          16       0.60      0.04      0.07       158\n",
      "          17       0.80      0.55      0.65       469\n",
      "          18       0.84      0.58      0.69      1583\n",
      "          19       0.79      0.55      0.65      1266\n",
      "          20       0.84      0.64      0.73       650\n",
      "          21       0.80      0.39      0.53       168\n",
      "          22       0.64      0.24      0.35       213\n",
      "          23       0.62      0.17      0.26       137\n",
      "          24       0.70      0.26      0.38       185\n",
      "          25       0.81      0.52      0.64       482\n",
      "          26       0.60      0.19      0.29       257\n",
      "          27       0.90      0.41      0.57       188\n",
      "          28       0.33      0.04      0.07       222\n",
      "          29       0.85      0.63      0.72      1121\n",
      "          30       0.87      0.68      0.76       939\n",
      "          31       0.81      0.45      0.58       235\n",
      "          32       0.73      0.40      0.51       300\n",
      "          33       0.89      0.59      0.71       347\n",
      "          34       0.50      0.05      0.09       149\n",
      "          35       0.65      0.28      0.39       632\n",
      "          36       0.73      0.40      0.51       493\n",
      "          37       0.48      0.07      0.13       175\n",
      "          38       0.94      0.69      0.80       191\n",
      "          39       0.81      0.33      0.47       162\n",
      "          40       0.36      0.03      0.06       129\n",
      "          41       0.67      0.27      0.39       220\n",
      "          42       0.50      0.21      0.29       289\n",
      "          43       0.39      0.05      0.09       185\n",
      "          44       0.80      0.20      0.33       181\n",
      "          45       0.42      0.06      0.10       136\n",
      "          46       0.54      0.13      0.20       383\n",
      "          47       0.64      0.18      0.28       234\n",
      "          48       0.89      0.60      0.72       295\n",
      "          49       0.78      0.30      0.43       285\n",
      "\n",
      "   micro avg       0.77      0.39      0.52     22081\n",
      "   macro avg       0.70      0.33      0.43     22081\n",
      "weighted avg       0.72      0.39      0.49     22081\n",
      " samples avg       0.51      0.43      0.45     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=LinearSVC(dual=False, random_state=5, tol=0.001))\n",
      "train time: 2.241s\n",
      "test time:  0.010s\n",
      "accuracy:   0.303\n",
      "f1-score (micro):   0.545\n",
      "f1-score (micro):   0.490\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.18      0.29      1358\n",
      "           1       0.64      0.43      0.51       196\n",
      "           2       0.70      0.24      0.35       178\n",
      "           3       0.96      0.71      0.81       507\n",
      "           4       0.54      0.26      0.35       188\n",
      "           5       0.77      0.45      0.57       928\n",
      "           6       0.81      0.58      0.68       260\n",
      "           7       0.71      0.25      0.36       412\n",
      "           8       0.63      0.20      0.30      2185\n",
      "           9       0.86      0.36      0.51       989\n",
      "          10       0.80      0.58      0.67       417\n",
      "          11       0.17      0.00      0.01       296\n",
      "          12       0.93      0.74      0.83       174\n",
      "          13       0.74      0.69      0.71       154\n",
      "          14       0.87      0.71      0.78       128\n",
      "          15       0.62      0.26      0.37       652\n",
      "          16       0.65      0.13      0.21       158\n",
      "          17       0.81      0.56      0.66       469\n",
      "          18       0.85      0.60      0.70      1583\n",
      "          19       0.78      0.52      0.62      1266\n",
      "          20       0.83      0.68      0.75       650\n",
      "          21       0.71      0.52      0.60       168\n",
      "          22       0.66      0.32      0.43       213\n",
      "          23       0.70      0.22      0.33       137\n",
      "          24       0.75      0.48      0.59       185\n",
      "          25       0.81      0.56      0.66       482\n",
      "          26       0.55      0.30      0.38       257\n",
      "          27       0.83      0.55      0.66       188\n",
      "          28       0.55      0.05      0.10       222\n",
      "          29       0.86      0.65      0.74      1121\n",
      "          30       0.89      0.65      0.75       939\n",
      "          31       0.86      0.64      0.73       235\n",
      "          32       0.72      0.40      0.52       300\n",
      "          33       0.88      0.66      0.76       347\n",
      "          34       0.58      0.10      0.17       149\n",
      "          35       0.64      0.26      0.37       632\n",
      "          36       0.71      0.41      0.52       493\n",
      "          37       0.64      0.04      0.08       175\n",
      "          38       0.94      0.82      0.87       191\n",
      "          39       0.83      0.56      0.67       162\n",
      "          40       0.62      0.04      0.07       129\n",
      "          41       0.68      0.41      0.51       220\n",
      "          42       0.58      0.24      0.34       289\n",
      "          43       1.00      0.01      0.01       185\n",
      "          44       0.73      0.43      0.54       181\n",
      "          45       0.55      0.09      0.15       136\n",
      "          46       0.52      0.06      0.11       383\n",
      "          47       0.72      0.26      0.39       234\n",
      "          48       0.91      0.69      0.78       295\n",
      "          49       0.75      0.54      0.63       285\n",
      "\n",
      "   micro avg       0.78      0.42      0.55     22081\n",
      "   macro avg       0.73      0.40      0.49     22081\n",
      "weighted avg       0.75      0.42      0.52     22081\n",
      " samples avg       0.50      0.45      0.45     22081\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=SGDClassifier(max_iter=50, n_jobs=-1,\n",
      "                                            random_state=3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.992s\n",
      "test time:  0.010s\n",
      "accuracy:   0.215\n",
      "f1-score (micro):   0.412\n",
      "f1-score (micro):   0.309\n",
      "dimensionality: 500\n",
      "density: 0.834160\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1358\n",
      "           1       0.00      0.00      0.00       196\n",
      "           2       0.00      0.00      0.00       178\n",
      "           3       0.97      0.60      0.74       507\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.77      0.39      0.52       928\n",
      "           6       0.81      0.46      0.59       260\n",
      "           7       0.86      0.11      0.19       412\n",
      "           8       0.00      0.00      0.00      2185\n",
      "           9       0.95      0.23      0.37       989\n",
      "          10       0.80      0.51      0.62       417\n",
      "          11       0.00      0.00      0.00       296\n",
      "          12       0.93      0.64      0.76       174\n",
      "          13       0.74      0.69      0.71       154\n",
      "          14       0.83      0.54      0.65       128\n",
      "          15       0.00      0.00      0.00       652\n",
      "          16       0.00      0.00      0.00       158\n",
      "          17       0.84      0.39      0.53       469\n",
      "          18       0.83      0.54      0.65      1583\n",
      "          19       0.79      0.42      0.55      1266\n",
      "          20       0.82      0.57      0.67       650\n",
      "          21       0.81      0.20      0.32       168\n",
      "          22       1.00      0.01      0.03       213\n",
      "          23       0.00      0.00      0.00       137\n",
      "          24       0.88      0.11      0.20       185\n",
      "          25       0.80      0.48      0.60       482\n",
      "          26       0.91      0.04      0.07       257\n",
      "          27       0.81      0.44      0.57       188\n",
      "          28       0.00      0.00      0.00       222\n",
      "          29       0.83      0.61      0.70      1121\n",
      "          30       0.89      0.56      0.69       939\n",
      "          31       0.87      0.47      0.61       235\n",
      "          32       0.75      0.36      0.49       300\n",
      "          33       0.87      0.52      0.65       347\n",
      "          34       0.00      0.00      0.00       149\n",
      "          35       0.00      0.00      0.00       632\n",
      "          36       0.80      0.15      0.25       493\n",
      "          37       0.00      0.00      0.00       175\n",
      "          38       0.93      0.74      0.83       191\n",
      "          39       0.88      0.33      0.48       162\n",
      "          40       0.00      0.00      0.00       129\n",
      "          41       0.68      0.15      0.24       220\n",
      "          42       0.00      0.00      0.00       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.55      0.03      0.06       181\n",
      "          45       0.00      0.00      0.00       136\n",
      "          46       0.00      0.00      0.00       383\n",
      "          47       1.00      0.01      0.02       234\n",
      "          48       0.95      0.58      0.72       295\n",
      "          49       0.83      0.25      0.39       285\n",
      "\n",
      "   micro avg       0.84      0.27      0.41     22081\n",
      "   macro avg       0.54      0.24      0.31     22081\n",
      "weighted avg       0.55      0.27      0.35     22081\n",
      " samples avg       0.36      0.30      0.31     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=LinearSVC(dual=False, penalty='l1',\n",
      "                                        random_state=5, tol=0.001))\n",
      "train time: 6.318s\n",
      "test time:  0.010s\n",
      "accuracy:   0.305\n",
      "f1-score (micro):   0.549\n",
      "f1-score (micro):   0.493\n",
      "dimensionality: 500\n",
      "density: 0.429960\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.19      0.29      1358\n",
      "           1       0.62      0.41      0.50       196\n",
      "           2       0.69      0.25      0.37       178\n",
      "           3       0.95      0.72      0.82       507\n",
      "           4       0.50      0.23      0.32       188\n",
      "           5       0.77      0.46      0.57       928\n",
      "           6       0.81      0.61      0.70       260\n",
      "           7       0.69      0.25      0.36       412\n",
      "           8       0.64      0.20      0.31      2185\n",
      "           9       0.85      0.37      0.51       989\n",
      "          10       0.79      0.58      0.67       417\n",
      "          11       0.17      0.00      0.01       296\n",
      "          12       0.93      0.77      0.84       174\n",
      "          13       0.74      0.69      0.71       154\n",
      "          14       0.85      0.72      0.78       128\n",
      "          15       0.62      0.26      0.37       652\n",
      "          16       0.69      0.14      0.23       158\n",
      "          17       0.82      0.57      0.67       469\n",
      "          18       0.85      0.61      0.71      1583\n",
      "          19       0.78      0.52      0.62      1266\n",
      "          20       0.83      0.69      0.76       650\n",
      "          21       0.71      0.52      0.60       168\n",
      "          22       0.66      0.33      0.44       213\n",
      "          23       0.73      0.26      0.38       137\n",
      "          24       0.73      0.47      0.57       185\n",
      "          25       0.81      0.56      0.66       482\n",
      "          26       0.55      0.30      0.38       257\n",
      "          27       0.83      0.57      0.68       188\n",
      "          28       0.55      0.05      0.10       222\n",
      "          29       0.86      0.66      0.75      1121\n",
      "          30       0.88      0.65      0.75       939\n",
      "          31       0.85      0.66      0.74       235\n",
      "          32       0.72      0.41      0.52       300\n",
      "          33       0.87      0.67      0.76       347\n",
      "          34       0.57      0.11      0.18       149\n",
      "          35       0.64      0.26      0.37       632\n",
      "          36       0.72      0.42      0.53       493\n",
      "          37       0.67      0.03      0.07       175\n",
      "          38       0.93      0.83      0.88       191\n",
      "          39       0.82      0.55      0.66       162\n",
      "          40       0.56      0.04      0.07       129\n",
      "          41       0.68      0.42      0.52       220\n",
      "          42       0.57      0.22      0.32       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.71      0.44      0.54       181\n",
      "          45       0.55      0.09      0.15       136\n",
      "          46       0.52      0.06      0.11       383\n",
      "          47       0.71      0.27      0.40       234\n",
      "          48       0.90      0.70      0.79       295\n",
      "          49       0.74      0.55      0.63       285\n",
      "\n",
      "   micro avg       0.78      0.42      0.55     22081\n",
      "   macro avg       0.71      0.41      0.49     22081\n",
      "weighted avg       0.73      0.42      0.52     22081\n",
      " samples avg       0.50      0.45      0.46     22081\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=SGDClassifier(max_iter=50, n_jobs=-1,\n",
      "                                            penalty='l1', random_state=3))\n",
      "train time: 1.325s\n",
      "test time:  0.010s\n",
      "accuracy:   0.247\n",
      "f1-score (micro):   0.470\n",
      "f1-score (micro):   0.378\n",
      "dimensionality: 500\n",
      "density: 0.028160\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.04      0.08      1358\n",
      "           1       0.67      0.17      0.27       196\n",
      "           2       0.75      0.13      0.23       178\n",
      "           3       0.96      0.64      0.77       507\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.74      0.47      0.57       928\n",
      "           6       0.68      0.55      0.61       260\n",
      "           7       0.77      0.17      0.28       412\n",
      "           8       0.64      0.01      0.03      2185\n",
      "           9       0.90      0.31      0.46       989\n",
      "          10       0.74      0.55      0.63       417\n",
      "          11       0.00      0.00      0.00       296\n",
      "          12       0.88      0.78      0.83       174\n",
      "          13       0.68      0.76      0.72       154\n",
      "          14       0.79      0.60      0.68       128\n",
      "          15       0.00      0.00      0.00       652\n",
      "          16       0.00      0.00      0.00       158\n",
      "          17       0.82      0.44      0.57       469\n",
      "          18       0.81      0.61      0.70      1583\n",
      "          19       0.74      0.46      0.57      1266\n",
      "          20       0.77      0.60      0.67       650\n",
      "          21       0.66      0.43      0.53       168\n",
      "          22       0.67      0.15      0.25       213\n",
      "          23       0.00      0.00      0.00       137\n",
      "          24       0.73      0.32      0.45       185\n",
      "          25       0.75      0.60      0.67       482\n",
      "          26       0.55      0.10      0.17       257\n",
      "          27       0.74      0.62      0.67       188\n",
      "          28       0.00      0.00      0.00       222\n",
      "          29       0.79      0.67      0.72      1121\n",
      "          30       0.85      0.65      0.74       939\n",
      "          31       0.83      0.60      0.69       235\n",
      "          32       0.68      0.43      0.52       300\n",
      "          33       0.83      0.59      0.69       347\n",
      "          34       0.00      0.00      0.00       149\n",
      "          35       0.00      0.00      0.00       632\n",
      "          36       0.77      0.26      0.39       493\n",
      "          37       0.00      0.00      0.00       175\n",
      "          38       0.87      0.84      0.86       191\n",
      "          39       0.81      0.43      0.56       162\n",
      "          40       0.00      0.00      0.00       129\n",
      "          41       0.62      0.37      0.46       220\n",
      "          42       0.00      0.00      0.00       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.60      0.19      0.29       181\n",
      "          45       0.00      0.00      0.00       136\n",
      "          46       0.00      0.00      0.00       383\n",
      "          47       0.71      0.19      0.30       234\n",
      "          48       0.84      0.67      0.75       295\n",
      "          49       0.74      0.42      0.53       285\n",
      "\n",
      "   micro avg       0.78      0.34      0.47     22081\n",
      "   macro avg       0.54      0.32      0.38     22081\n",
      "weighted avg       0.63      0.34      0.40     22081\n",
      " samples avg       0.42      0.36      0.37     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=SGDClassifier(max_iter=50, n_jobs=-1,\n",
      "                                            penalty='elasticnet',\n",
      "                                            random_state=5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.854s\n",
      "test time:  0.010s\n",
      "accuracy:   0.216\n",
      "f1-score (micro):   0.416\n",
      "f1-score (micro):   0.314\n",
      "dimensionality: 500\n",
      "density: 0.193960\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1358\n",
      "           1       0.00      0.00      0.00       196\n",
      "           2       0.00      0.00      0.00       178\n",
      "           3       0.97      0.61      0.75       507\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.76      0.39      0.52       928\n",
      "           6       0.78      0.47      0.58       260\n",
      "           7       0.84      0.11      0.20       412\n",
      "           8       0.50      0.00      0.00      2185\n",
      "           9       0.94      0.23      0.37       989\n",
      "          10       0.79      0.51      0.62       417\n",
      "          11       0.00      0.00      0.00       296\n",
      "          12       0.91      0.66      0.76       174\n",
      "          13       0.73      0.70      0.72       154\n",
      "          14       0.80      0.54      0.64       128\n",
      "          15       0.00      0.00      0.00       652\n",
      "          16       0.00      0.00      0.00       158\n",
      "          17       0.83      0.39      0.53       469\n",
      "          18       0.82      0.55      0.66      1583\n",
      "          19       0.79      0.42      0.55      1266\n",
      "          20       0.82      0.58      0.68       650\n",
      "          21       0.79      0.22      0.34       168\n",
      "          22       1.00      0.01      0.03       213\n",
      "          23       0.00      0.00      0.00       137\n",
      "          24       0.88      0.12      0.21       185\n",
      "          25       0.79      0.50      0.61       482\n",
      "          26       0.82      0.04      0.07       257\n",
      "          27       0.81      0.45      0.58       188\n",
      "          28       0.00      0.00      0.00       222\n",
      "          29       0.81      0.61      0.70      1121\n",
      "          30       0.88      0.57      0.69       939\n",
      "          31       0.88      0.49      0.63       235\n",
      "          32       0.74      0.37      0.49       300\n",
      "          33       0.85      0.52      0.64       347\n",
      "          34       0.00      0.00      0.00       149\n",
      "          35       0.00      0.00      0.00       632\n",
      "          36       0.80      0.15      0.25       493\n",
      "          37       0.00      0.00      0.00       175\n",
      "          38       0.92      0.75      0.83       191\n",
      "          39       0.86      0.33      0.48       162\n",
      "          40       0.00      0.00      0.00       129\n",
      "          41       0.68      0.17      0.28       220\n",
      "          42       0.00      0.00      0.00       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       0.65      0.06      0.11       181\n",
      "          45       0.00      0.00      0.00       136\n",
      "          46       0.00      0.00      0.00       383\n",
      "          47       0.86      0.03      0.05       234\n",
      "          48       0.93      0.59      0.72       295\n",
      "          49       0.83      0.26      0.40       285\n",
      "\n",
      "   micro avg       0.83      0.28      0.42     22081\n",
      "   macro avg       0.54      0.25      0.31     22081\n",
      "weighted avg       0.59      0.28      0.35     22081\n",
      " samples avg       0.36      0.30      0.32     22081\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01))\n",
      "train time: 0.299s\n",
      "test time:  0.077s\n",
      "accuracy:   0.042\n",
      "f1-score (micro):   0.100\n",
      "f1-score (micro):   0.087\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1358\n",
      "           1       0.00      0.00      0.00       196\n",
      "           2       0.00      0.00      0.00       178\n",
      "           3       0.99      0.21      0.34       507\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.93      0.01      0.03       928\n",
      "           6       0.95      0.07      0.14       260\n",
      "           7       0.93      0.07      0.13       412\n",
      "           8       0.33      0.00      0.01      2185\n",
      "           9       0.89      0.13      0.23       989\n",
      "          10       0.84      0.21      0.33       417\n",
      "          11       0.00      0.00      0.00       296\n",
      "          12       0.97      0.18      0.31       174\n",
      "          13       0.67      0.01      0.03       154\n",
      "          14       0.94      0.12      0.22       128\n",
      "          15       0.69      0.10      0.18       652\n",
      "          16       0.56      0.06      0.10       158\n",
      "          17       0.78      0.18      0.29       469\n",
      "          18       0.86      0.08      0.15      1583\n",
      "          19       0.69      0.06      0.10      1266\n",
      "          20       0.92      0.07      0.13       650\n",
      "          21       0.00      0.00      0.00       168\n",
      "          22       0.00      0.00      0.00       213\n",
      "          23       0.00      0.00      0.00       137\n",
      "          24       1.00      0.01      0.02       185\n",
      "          25       0.89      0.02      0.03       482\n",
      "          26       0.38      0.08      0.13       257\n",
      "          27       1.00      0.02      0.03       188\n",
      "          28       0.00      0.00      0.00       222\n",
      "          29       0.91      0.04      0.07      1121\n",
      "          30       0.91      0.07      0.14       939\n",
      "          31       0.83      0.02      0.04       235\n",
      "          32       0.64      0.05      0.09       300\n",
      "          33       0.84      0.08      0.14       347\n",
      "          34       0.00      0.00      0.00       149\n",
      "          35       0.70      0.01      0.02       632\n",
      "          36       0.60      0.01      0.01       493\n",
      "          37       0.00      0.00      0.00       175\n",
      "          38       0.98      0.28      0.43       191\n",
      "          39       1.00      0.02      0.05       162\n",
      "          40       0.00      0.00      0.00       129\n",
      "          41       1.00      0.00      0.01       220\n",
      "          42       0.00      0.00      0.00       289\n",
      "          43       0.00      0.00      0.00       185\n",
      "          44       1.00      0.03      0.05       181\n",
      "          45       0.00      0.00      0.00       136\n",
      "          46       0.00      0.00      0.00       383\n",
      "          47       0.00      0.00      0.00       234\n",
      "          48       0.94      0.20      0.33       295\n",
      "          49       0.83      0.02      0.03       285\n",
      "\n",
      "   micro avg       0.83      0.05      0.10     22081\n",
      "   macro avg       0.55      0.05      0.09     22081\n",
      "weighted avg       0.61      0.05      0.09     22081\n",
      " samples avg       0.07      0.06      0.06     22081\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=BernoulliNB(alpha=0.01))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.364s\n",
      "test time:  0.100s\n",
      "accuracy:   0.154\n",
      "f1-score (micro):   0.495\n",
      "f1-score (micro):   0.483\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.51      0.39      1358\n",
      "           1       0.14      0.61      0.23       196\n",
      "           2       0.37      0.56      0.44       178\n",
      "           3       0.78      0.68      0.73       507\n",
      "           4       0.23      0.69      0.34       188\n",
      "           5       0.51      0.65      0.57       928\n",
      "           6       0.39      0.71      0.51       260\n",
      "           7       0.29      0.52      0.38       412\n",
      "           8       0.44      0.53      0.48      2185\n",
      "           9       0.48      0.60      0.53       989\n",
      "          10       0.37      0.66      0.47       417\n",
      "          11       0.14      0.56      0.23       296\n",
      "          12       0.55      0.76      0.64       174\n",
      "          13       0.41      0.83      0.55       154\n",
      "          14       0.67      0.84      0.75       128\n",
      "          15       0.30      0.56      0.39       652\n",
      "          16       0.27      0.61      0.37       158\n",
      "          17       0.67      0.77      0.72       469\n",
      "          18       0.69      0.72      0.70      1583\n",
      "          19       0.50      0.64      0.56      1266\n",
      "          20       0.39      0.73      0.51       650\n",
      "          21       0.27      0.74      0.40       168\n",
      "          22       0.25      0.58      0.35       213\n",
      "          23       0.25      0.61      0.36       137\n",
      "          24       0.45      0.70      0.55       185\n",
      "          25       0.38      0.80      0.51       482\n",
      "          26       0.38      0.64      0.48       257\n",
      "          27       0.20      0.79      0.32       188\n",
      "          28       0.23      0.33      0.28       222\n",
      "          29       0.60      0.68      0.64      1121\n",
      "          30       0.81      0.75      0.78       939\n",
      "          31       0.62      0.80      0.70       235\n",
      "          32       0.53      0.65      0.59       300\n",
      "          33       0.71      0.74      0.72       347\n",
      "          34       0.22      0.54      0.31       149\n",
      "          35       0.39      0.81      0.53       632\n",
      "          36       0.35      0.83      0.50       493\n",
      "          37       0.15      0.49      0.23       175\n",
      "          38       0.67      0.91      0.77       191\n",
      "          39       0.53      0.81      0.64       162\n",
      "          40       0.13      0.35      0.19       129\n",
      "          41       0.45      0.65      0.53       220\n",
      "          42       0.27      0.62      0.38       289\n",
      "          43       0.16      0.51      0.24       185\n",
      "          44       0.30      0.70      0.42       181\n",
      "          45       0.16      0.52      0.25       136\n",
      "          46       0.24      0.60      0.34       383\n",
      "          47       0.29      0.66      0.41       234\n",
      "          48       0.70      0.75      0.72       295\n",
      "          49       0.46      0.61      0.52       285\n",
      "\n",
      "   micro avg       0.40      0.65      0.50     22081\n",
      "   macro avg       0.40      0.66      0.48     22081\n",
      "weighted avg       0.46      0.65      0.52     22081\n",
      " samples avg       0.42      0.66      0.47     22081\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "OneVsRestClassifier(estimator=ComplementNB(alpha=0.1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.299s\n",
      "test time:  0.081s\n",
      "accuracy:   0.008\n",
      "f1-score (micro):   0.281\n",
      "f1-score (micro):   0.268\n",
      "dimensionality: 500\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.79      0.36      1358\n",
      "           1       0.07      0.94      0.12       196\n",
      "           2       0.06      0.84      0.11       178\n",
      "           3       0.32      0.91      0.48       507\n",
      "           4       0.07      0.94      0.14       188\n",
      "           5       0.24      0.86      0.38       928\n",
      "           6       0.13      0.89      0.22       260\n",
      "           7       0.13      0.75      0.22       412\n",
      "           8       0.33      0.81      0.47      2185\n",
      "           9       0.27      0.80      0.41       989\n",
      "          10       0.24      0.93      0.38       417\n",
      "          11       0.10      0.85      0.17       296\n",
      "          12       0.15      0.94      0.26       174\n",
      "          13       0.08      0.93      0.15       154\n",
      "          14       0.18      0.93      0.30       128\n",
      "          15       0.22      0.85      0.34       652\n",
      "          16       0.13      0.89      0.23       158\n",
      "          17       0.40      0.92      0.56       469\n",
      "          18       0.40      0.86      0.54      1583\n",
      "          19       0.35      0.88      0.50      1266\n",
      "          20       0.23      0.94      0.37       650\n",
      "          21       0.07      0.96      0.13       168\n",
      "          22       0.07      0.87      0.13       213\n",
      "          23       0.05      0.84      0.09       137\n",
      "          24       0.09      0.94      0.17       185\n",
      "          25       0.20      0.95      0.33       482\n",
      "          26       0.24      0.90      0.37       257\n",
      "          27       0.10      0.93      0.17       188\n",
      "          28       0.04      0.72      0.08       222\n",
      "          29       0.28      0.90      0.43      1121\n",
      "          30       0.37      0.89      0.53       939\n",
      "          31       0.18      0.94      0.30       235\n",
      "          32       0.19      0.88      0.32       300\n",
      "          33       0.23      0.90      0.37       347\n",
      "          34       0.04      0.81      0.07       149\n",
      "          35       0.28      0.94      0.43       632\n",
      "          36       0.22      0.95      0.36       493\n",
      "          37       0.05      0.87      0.10       175\n",
      "          38       0.22      0.96      0.36       191\n",
      "          39       0.13      0.98      0.22       162\n",
      "          40       0.03      0.78      0.06       129\n",
      "          41       0.07      0.85      0.12       220\n",
      "          42       0.09      0.84      0.17       289\n",
      "          43       0.05      0.82      0.10       185\n",
      "          44       0.09      0.87      0.16       181\n",
      "          45       0.06      0.88      0.10       136\n",
      "          46       0.10      0.86      0.18       383\n",
      "          47       0.10      0.89      0.17       234\n",
      "          48       0.23      0.94      0.36       295\n",
      "          49       0.16      0.93      0.28       285\n",
      "\n",
      "   micro avg       0.17      0.87      0.28     22081\n",
      "   macro avg       0.17      0.88      0.27     22081\n",
      "weighted avg       0.24      0.87      0.36     22081\n",
      " samples avg       0.20      0.88      0.31     22081\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(steps=[('feature_selection',\n",
      "                 SelectFromModel(estimator=OneVsRestClassifier(estimator=LinearSVC(dual=False,\n",
      "                                                                                   penalty='l1',\n",
      "                                                                                   random_state=4,\n",
      "                                                                                   tol=0.001)))),\n",
      "                ('classification',\n",
      "                 OneVsRestClassifier(estimator=LinearSVC(random_state=8)))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). If you observe this warning while using RFE or SelectFromModel, use the importance_getter parameter instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 8.054s\n",
      "test time:  0.010s\n",
      "accuracy:   0.273\n",
      "f1-score (micro):   0.511\n",
      "f1-score (micro):   0.447\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.18      0.28      1358\n",
      "           1       0.63      0.38      0.48       196\n",
      "           2       0.69      0.22      0.34       178\n",
      "           3       0.96      0.65      0.77       507\n",
      "           4       0.52      0.24      0.33       188\n",
      "           5       0.76      0.43      0.55       928\n",
      "           6       0.78      0.50      0.61       260\n",
      "           7       0.72      0.18      0.29       412\n",
      "           8       0.62      0.17      0.26      2185\n",
      "           9       0.87      0.34      0.49       989\n",
      "          10       0.73      0.46      0.56       417\n",
      "          11       0.17      0.00      0.01       296\n",
      "          12       0.92      0.72      0.81       174\n",
      "          13       0.73      0.67      0.70       154\n",
      "          14       0.80      0.54      0.64       128\n",
      "          15       0.56      0.18      0.28       652\n",
      "          16       0.58      0.07      0.12       158\n",
      "          17       0.80      0.50      0.61       469\n",
      "          18       0.84      0.58      0.69      1583\n",
      "          19       0.76      0.49      0.60      1266\n",
      "          20       0.81      0.61      0.70       650\n",
      "          21       0.69      0.45      0.54       168\n",
      "          22       0.57      0.23      0.33       213\n",
      "          23       0.65      0.16      0.26       137\n",
      "          24       0.71      0.38      0.50       185\n",
      "          25       0.82      0.52      0.64       482\n",
      "          26       0.57      0.29      0.39       257\n",
      "          27       0.81      0.50      0.62       188\n",
      "          28       0.55      0.05      0.09       222\n",
      "          29       0.85      0.63      0.72      1121\n",
      "          30       0.89      0.64      0.74       939\n",
      "          31       0.87      0.50      0.63       235\n",
      "          32       0.72      0.39      0.51       300\n",
      "          33       0.87      0.66      0.75       347\n",
      "          34       0.50      0.07      0.13       149\n",
      "          35       0.60      0.22      0.32       632\n",
      "          36       0.69      0.34      0.46       493\n",
      "          37       0.80      0.02      0.04       175\n",
      "          38       0.89      0.78      0.83       191\n",
      "          39       0.75      0.40      0.52       162\n",
      "          40       0.57      0.03      0.06       129\n",
      "          41       0.67      0.38      0.49       220\n",
      "          42       0.55      0.21      0.30       289\n",
      "          43       1.00      0.01      0.01       185\n",
      "          44       0.72      0.37      0.49       181\n",
      "          45       0.54      0.05      0.09       136\n",
      "          46       0.43      0.04      0.08       383\n",
      "          47       0.73      0.24      0.37       234\n",
      "          48       0.90      0.64      0.75       295\n",
      "          49       0.74      0.46      0.56       285\n",
      "\n",
      "   micro avg       0.77      0.38      0.51     22081\n",
      "   macro avg       0.71      0.36      0.45     22081\n",
      "weighted avg       0.73      0.38      0.48     22081\n",
      " samples avg       0.46      0.41      0.41     22081\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). If you observe this warning while using RFE or SelectFromModel, use the importance_getter parameter instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAI1CAYAAADLgluYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABXiklEQVR4nO3deZhdVZX38e8vIZAABSggElEDyCQJBCrgCBaDiII44dTYDM6gKA60dEsLiNoojjhhqwgqKM4g2jIopYBMVRAgzKgREV8EFCwwARLW+8c9iUVRSVWSm9wM38/z3Cf37LPP3uvcK5jF2mffVBWSJEmSpCU3ptMBSJIkSdLKwgRLkiRJktrEBEuSJEmS2sQES5IkSZLaxARLkiRJktrEBEuSJEmS2sQES5IkSZLaxARLkqSVXJLnJ/ltkvuT/C3JJUl26nRckrQyWq3TAUiSpKUnyTrAOcChwPeA1YFdgIfaOMfYqprbrvEkaUVmBUuSpJXblgBV9Z2qmltVs6rqvKq6FiDJW5LcmGQgyQ1Jdmzat0nSm+S+JNcn2W/egElOTfLlJD9P8iCwW5KJSX6Y5O4kf0jyro7crSR1mAmWJEkrt1uAuUlOS/LiJE+YdyLJq4FjgQOBdYD9gHuTjAN+CpwHPAk4HDg9yVaDxv034KNAF/Dbpv81wFOAPYAjkrxoKd+bJC13TLAkSVqJVdU/gOcDBXwVuDvJ2Uk2At4MfKKqrqyW26rqj8CzgbWBE6rq4ar6Fa1lhq8fNPRZVXVJVT0KTAE2rKoPN/1/38z1umV3p5K0fPAZLEmSVnJVdSNwMECSrYFvA58Fngr8bphLJgJ/apKnef5Iqzo1z58GvX86MDHJfYPaxgIXLWHokrTCMcGSJGkVUlU3JTkVeButJGnzYbrdCTw1yZhBSdbTaC03nD/UoPd/Av5QVVsshZAlaYXiEkFJklZiSbZO8r4kmzTHT6W11O8y4GvA+5N0p+UZSZ4OXA78E/iPJOOS9AAvBb67gGmuAAaSfCDJhCRjk0x2K3hJqyITLEmSVm4DwLOAy5sd/y4DZgDvq6rv09qo4oym30+AJ1bVw7QSqhcD9wBfAg6sqpuGm6DZon1fYCrwh+aarwHrLrW7kqTlVKpq5F6SJEmSpBFZwZIkSZKkNjHBkiRJkqQ2McGSJEmSpDYxwZIkSZKkNvF3sCRggw02qEmTJnU6DEmSJK0g+vv776mqDYe2m2BJwKRJk+jr6+t0GJIkSVpBJPnjcO0uEZQkSZKkNjHBkiRJkqQ2McGSJEmSpDbxGSxJkiRpOfLII49wxx13MHv27E6HImD8+PFssskmjBs3blT9TbAkSZKk5cgdd9xBV1cXkyZNIkmnw1mlVRX33nsvd9xxB5tuuumornGJoCRJkrQcmT17Nuuvv77J1XIgCeuvv/4iVRNNsCRJkqTljMnV8mNRvwsTLEmSJElqE5/BkiRJkpZjyXFtHa/qmLaOp8eygiVJkiRpqZkzZ06nQ1imTLAkSZIkPcaDDz7IPvvsw/bbb8/kyZM588wzufLKK3nuc5/L9ttvz84778zAwACzZ8/mkEMOYcqUKeywww5ceOGFAJx66qnst99+7L777uyxxx48+OCDvPGNb2TnnXdmhx124KyzzurwHS49LhGUJEmS9Bi/+MUvmDhxIj/72c8AuP/++9lhhx0488wz2WmnnfjHP/7BhAkT+NznPkcSrrvuOm666Sb22msvbrnlFgCuuuoqrr32Wp74xCfyX//1X+y+++6ccsop3Hfffey8887sueeerLXWWp28zaXCCpYkSZKkx5gyZQrnn38+H/jAB7jooou4/fbb2Xjjjdlpp50AWGeddVhttdW4+OKLecMb3gDA1ltvzdOf/vT5CdYLX/hCnvjEJwJw3nnnccIJJzB16lR6enqYPXs2t99+e2dubimzgiVJkiTpMbbcckuuuuoqfv7zn3P00Uez++67L/IYg6tTVcUPf/hDttpqq3aGuVyygiVJkiTpMe68807WXHNN3vCGN3DkkUdy+eWX85e//IUrr7wSgIGBAebMmcMuu+zC6aefDsAtt9zC7bffPmwS9aIXvYjPf/7zVBUAV1999bK7mWXMCpYkSZK0HOvEturXXXcdRx55JGPGjGHcuHF8+ctfpqo4/PDDmTVrFhMmTOCCCy7gsMMO49BDD2XKlCmsttpqnHrqqayxxhqPG++///u/OeKII9huu+149NFH2XTTTTnnnHOW+X0tC5mXRUqrsmnTplVfX1+nw5AkSeLGG29km2226XQYGmS47yRJf1VNG9rXJYKSJEmS1CYmWJIkSZLUJiZYkiRJktQmJliSJEmS1CYmWJIkSZLUJiZYEsBd/Z2OQJIkSSsBfwdLkiRJWo4l7R3PX2lauqxgSZIkSXqMsWPHMnXq1PmvmTNncu+997Lbbrux9tpr8853vnOh1++///78/ve/H/V8fX19vOtd71rSsHn44YfZddddmTNnzhKPtbisYEkAG3V3OgJJkqTlxoQJE5g+ffpj2h588EGOP/54ZsyYwYwZMxZ47fXXX8/cuXPZbLPNRj3ftGnTmDbtcb/Zu0Bz585l7Nixj2tfffXV2WOPPTjzzDM54IADRj1eO1nBkiRJkjSitdZai+c///mMHz9+of1OP/10Xvayl80/XnvttTnyyCPZdttt2XPPPbniiivo6elhs8024+yzzwagt7eXfffdF4AHHniAQw45hClTprDddtvxwx/+cP4473vf+9h+++259NJL+fSnP83kyZOZPHkyn/3sZ+fP9/KXv5zTTz+9zXc/eiZYkiRJkh5j1qxZ85cHvuIVr1ikay+55BK6u/+1OujBBx9k99135/rrr6erq4ujjz6a888/nx//+Md86EMfetz1xx9/POuuuy7XXXcd1157Lbvvvvv8cZ71rGdxzTXXMGHCBL7xjW9w+eWXc9lll/HVr36Vq6++GoDJkydz5ZVXLsHdLxmXCEqSJEl6jOGWCI7WX/7yFzbccMP5x6uvvjp77703AFOmTGGNNdZg3LhxTJkyhZkzZz7u+gsuuIDvfve784+f8IQnAK3nwl71qlcBcPHFF/OKV7yCtdZaC4BXvvKVXHTRReywww6MHTuW1VdfnYGBAbq6uhbrHpaECZYE9A8MkN7eTochSaNWPT2dDkGShjVhwgRmz549/3jcuHGk2QpxzJgxrLHGGvPfL8pmFOPHjx/2uavhPPTQQyMuZVxaXCIoSZIkLceq2vta2rbZZhtuu+22xb7+hS98IV/84hfnH//9739/XJ9ddtmFn/zkJ/zzn//kwQcf5Mc//jG77LILAPfeey8bbLAB48aNW+wYloQJliRJkqRRmTRpEu9973s59dRT2WSTTbjhhhse12efffahdwlWBh199NH8/e9/Z/LkyWy//fZceOGFj+uz4447cvDBB7PzzjvzrGc9ize/+c3ssMMOAFx44YXss88+iz3/kkr5S2MS06ZNq76+vk6HIUmSxI033sg222zT6TAW26xZs9htt9245JJLRr2kr51e+cpXcsIJJ7Dlllu2bczhvpMk/VX1uL3lrWBJkiRJapsJEyZw3HHH8ec//3mZz/3www/z8pe/vK3J1aJykwtJkiRJbfWiF72oI/OuvvrqHHjggR2Zex4rWJIkSZLUJlawJGCgf4De9HY6DEkrmZ7q6XQIkqRlzAqWJEmSJLWJFSxJkiRpOdbuVTZW15cuEywJ6Oruoqevp9NhSJIkLRfGjh3LlClT5h//5Cc/oauri/33358rr7ySgw8+mC984QsLvH7//ffnE5/4BJttttmyCHeBXve613H88cezxRZbLLM5TbAkSZIkPcaECROYPn36Y9oefPBBjj/+eGbMmMGMGTMWeO3111/P3Llzl1lyNWfOHFZbbfi05tBDD+UTn/gEX/3qV5dJLOAzWJIkSZJGYa211uL5z38+48ePX2i/008/nZe97GXzj9dee22OPPJItt12W/bcc0+uuOIKenp62GyzzTj77LMBmDlzJrvssgs77rgjO+64I7/97W/nX//xj3+cKVOmsP3223PUUUcB0NPTwxFHHMG0adP43Oc+xy9/+Ut22GEHpkyZwhvf+EYeeughAHbZZRcuuOAC5syZ0+6PY4GsYElAfz8knY5Ckpatqk5HIGl5NWvWLKZOnQrApptuyo9//ONRX3vJJZfw+te/fv7xgw8+yO67786JJ57IK17xCo4++mjOP/98brjhBg466CD2228/nvSkJ3H++eczfvx4br31Vl7/+tfT19fH//3f/3HWWWdx+eWXs+aaa/K3v/1t/rgPP/wwfX19zJ49my222IJf/vKXbLnllhx44IF8+ctf5ogjjmDMmDE84xnP4JprrqG7u7ttn8/CmGBJkiRJeozhlgiO1l/+8hc23HDD+cerr746e++9NwBTpkxhjTXWYNy4cUyZMoWZM2cC8Mgjj/DOd76T6dOnM3bsWG655RYALrjgAg455BDWXHNNAJ74xCfOH/e1r30tADfffDObbropW265JQAHHXQQX/ziFzniiCMAeNKTnsSdd965zBKsEZcIJvlgkuuTXJtkepJnJTkmyf8M6Tc1yY3N+7WTfCXJ75L0J+lN8qxhxp6ZZIMhbVsnuTTJQ0nev5C4Xp3kxiQXjv5251+7XpLDFvW6pSFJT5JzFvPaI5Ks2e6YJEmSpMU1YcIEZs+ePf943LhxpFkqNGbMGNZYY4357+ct3fvMZz7DRhttxDXXXENfXx8PP/zwiPOstdZao4pn9uzZTJgwYVFvY7EttIKV5DnAvsCOVfVQkwytDnwH+AXwn4O6v65pB/ga8Adgi6p6NMmmwDNHGdPfgHcBLx+h35uAt1TVxaMcd7D1gMOALy3KRUnGVtXcxZhvaTkC+Dbwzw7HIUmSpKVkRdtWfZtttuG2225j0qRJo77m/vvvZ5NNNmHMmDGcdtppzJ3b+iv3C1/4Qj784Q9zwAEHzF8iOLiKBbDVVlsxc+ZMbrvtNp7xjGfwrW99ixe84AXzz99yyy1Mnjy5Lfc2GiNVsDYG7qmqhwCq6p6qurOqbgH+PqQq9RrgO0k2B54FHF1VjzbX/aGqfjaagKrqr1V1JfDIgvok+RDwfODrSU5MMrb588qm0va2pt/aSX6Z5Kok1yWZ97TdCcDmTUXuxKFVpCRfSHJw835mko8nuQp4dZK9mgrbVUm+n2TtYeJ7V5Ibmli+27StleSUJFckuXpQLIOvG7ZPc3+fTDKjGfPwJO8CJgIXLk4VT4/V3d16FsGXL1++VqWXJC2qSZMm8d73vpdTTz2VTTbZhBtuuOFxffbZZx96e3sXadzDDjuM0047je23356bbrppfnVq7733Zr/99mPatGlMnTqVT37yk4+7dvz48XzjG9/g1a9+NVOmTGHMmDG8/e1vB+Cuu+5iwoQJPPnJT170m11MqYX8G7ZJHi4G1gQuAM6sql83594PPKWq3pPk2cAXqmpakv2AQ6rqFSNOnswEplXVPcOcOxZ4oKoe/ym2zvcC76+qviRvBZ5UVR9JsgZwCfBq4E/AmlX1j6b6dhmwBfB04JyqmtyM1dOMtW9z/AWgr6pObWL8UlV9ohnjR8CLq+rBJB8A1qiqDw+J7U5g06bqt15V3ZfkY8ANVfXtJOsBVwA7ADvNm3shfQ4E9gBeV1Vzkjyxqv62sM9Pi2batGnV19fX6TAkSZK48cYb2WabbTodxmKbNWsWu+22G5dccgljx47taCyf+cxnWGeddXjTm960ROMM950k6a+qaUP7LrSCVVUPAN3AW4G7gTPnVXaAM4H9k4zhscsDO2Ev4MAk04HLgfVpJVIBPpbkWloJ4lOAjRZj/DObP59Na6njJc1cB9FK1oa6Fjg9yRuAeXtC7gUc1VzXC4wHnjbMfQzXZ0/gK1U1B6Cq/oYkSZK0HJowYQLHHXccf/7znzsdCuuttx4HHXTQMp1zxF0Em2eOeoHeJNfRSipOrao/JfkD8ALgVcBzmkuuB7Zfxs8rBTi8qs59TGMrGdwQ6K6qR5qKz3Ab98/hscnm0D4PDprn/Kp6PQu3D7Ar8FLgg0mmNNe+qqpuHhLj4IRvQX1GmE5Lqr//TpLjOh2GNKKqYzodgiRJI3rRi17U6RAAOOSQQ5b5nAutYCXZKskWg5qmAn8cdPwd4DPA76vqDoCq+h3QBxyXJjNIMinJPu0MfIhzgUOTjGvm2zLJWsC6wF+b5Go3/lVtGgC6Bl3/R+CZSdZolubtsYB5LgOel+QZzTxrJdlycIemovfUqroQ+EATw9pNjIcP+kx2WMB9DNfnfOBtSVZr2uc92Tf0PiRJkiR10EibXKwNnDZvwwZay+OOHXT++8C2PH554JtpLcW7LckM4FTgrwuY49okdzSvTyd5cpI7gPcCRzft64wQ59eAG4Crmvm+Qqs6dzowram8HQjcBFBV99Ja5jcjyYlV9Sfge8CM5s+rh5ukqu4GDqa1mce1wKXA1kO6jQW+3cx5NXBSVd0HHA+Ma+73+uZ4qAX1+Rpwe9N+DfBvTfv/Ar9wkwtJkiRp+bDQTS6kVYWbXEiSpOXFir7JxcpoUTa5GPEZLEmSJEmdk0Xc8nwk1dOz0PP33XcfZ5xxBocddtgij/2Sl7yEM844g/XWW2+BfT70oQ+x6667sueeey7y+EN97GMf47/+67/mHz/3uc/lt7/97RKPuySsYElYwZIkScuPodWSZZ1gzZw5k3333ZcZM2Y87tycOXNYbbXlp0az9tpr88ADDyz1edq2TbskSZKkVctRRx3F7373O6ZOncqRRx5Jb28vu+yyC/vttx/PfOYzAXj5y19Od3c32267Lf/7v/87/9pJkyZxzz33MHPmTLbZZhve8pa3sO2227LXXnsxa9YsAA4++GB+8IMfzO9/zDHHsOOOOzJlyhRuuukmAO6++25e+MIXsu222/LmN7+Zpz/96dxzzz2Pi3PWrFlMnTqVAw44AGglXAC9vb284AUv4GUvexmbbbYZRx11FKeffjo777wzU6ZM4Xe/+938eV71qlex0047sdNOO3HJJZcs8edngiUB3NUPn0rrJUmStAo74YQT2HzzzZk+fTonnngiAFdddRWf+9znuOWWWwA45ZRT6O/vp6+vj5NOOol77733cePceuutvOMd7+D6669nvfXW44c//OGw822wwQZcddVVHHrooXzyk58E4LjjjmP33Xfn+uuvZ//99+f2228fNs4JEyYwffp0Tj/99Medv+aaazj55JO58cYb+da3vsUtt9zCFVdcwZvf/GY+//nPA/Dud7+b97znPVx55ZX88Ic/5M1vfvPifWiDLD/1PUmSJEnLpZ133plNN910/vFJJ53Ej3/8YwD+9Kc/ceutt7L++us/5ppNN92UqVOnAtDd3c3MmTOHHfuVr3zl/D4/+tGPALj44ovnj7/33nvzhCc8YZFj3mmnndh4440B2Hzzzdlrr70AmDJlChde2NqE+4ILLuCGG26Yf80//vEPHnjggfmVsMVhgiVJkiRpodZaa63573t7e7ngggu49NJLWXPNNenp6WH27NmPu2aNNdaY/37s2LHzlwguqN/YsWOZM2dO22IePP+YMWPmH48ZM2b+PI8++iiXXXYZ48ePb9u8LhGUADbqhvdV6yVJkrQK6+rqYmBgYIHn77//fp7whCew5pprctNNN3HZZZe1PYbnPe95fO973wPgvPPO4+9///uw/caNG8cjjzyy2PPstdde85cLAkyfPn2xx5rHCpYkSZK0HBtp1792W3/99Xne857H5MmTefGLX8w+++zzmPN77703J598Mttssw1bbbUVz372s9sewzHHHMPrX/96vvWtb/Gc5zyHJz/5yXR1dT2u31vf+la22247dtxxx2GfwxrJSSedxDve8Q6222475syZw6677srJJ5+8RLG7TbuE27RLkqTlhz80DA899BBjx45ltdVW49JLL+XQQw9tS3VpcflDw5IkSZJWWLfffjuvec1rePTRR1l99dX56le/2umQRs0ESwL6BwYW6Uf8lnWpXpIkaVWyxRZbcPXVV3c6jMXiJheSJEmS1CYmWJIkSZLUJi4RlIDuri76XPYnSZKkJWQFS5IkSZLaxAqWJEmStDz7VNo73vsW/jNN9913H2eccQaHHXbYYg3/2c9+lre+9a2sueaaI557yUtewhlnnMF66623WHMtj/wdLAnYKlvVV/hKp8OQhtVTPZ0OQZK0DD3uN5eWcYI1c+ZM9t13X2bMmLFYw0+aNIm+vj422GCDRTq3PFuU38FyiaAkSZKk+Y466ih+97vfMXXqVI488kgATjzxRHbaaSe22247jjnmGAAefPBB9tlnH7bffnsmT57MmWeeyUknncSdd97Jbrvtxm677faYcYc7N2nSJO655x5mzpzJ1ltvzcEHH8yWW27JAQccwAUXXMDznvc8tthiC6644or5c77xjW9k5513ZocdduCss85ahp/M6LhEUJIkSdJ8J5xwAjNmzGD69OkAnHfeedx6661cccUVVBX77bcfv/nNb7j77ruZOHEiP/vZzwC4//77WXfddfn0pz/NhRde+Lgq1bve9a4FngO47bbb+P73v88pp5zCTjvtxBlnnMHFF1/M2Wefzcc+9jF+8pOf8NGPfpTdd9+dU045hfvuu4+dd96ZPffck7XWWmupfy6jZQVLkiRJ0gKdd955nHfeeeywww7suOOO3HTTTdx6661MmTKF888/nw984ANcdNFFrLvuuks0z6abbsqUKVMYM2YM2267LXvssQdJmDJlCjNnzpwfywknnMDUqVPp6elh9uzZ3H777W24y/axgiUBXd1d9PT1dDoMSZKk5U5V8Z//+Z+87W1ve9y5q666ip///OccffTR7LHHHnzoQx9a7HnWWGON+e/HjBkz/3jMmDHMmTNnfiw//OEP2WqrrRZ7nqXNCpYkSZKk+bq6uhgYGJh//KIXvYhTTjmFBx54AIA///nP/PWvf+XOO+9kzTXX5A1veANHHnkkV1111bDXL2zsRfWiF72Iz3/+88zbqO/qq69e7LGWFitYkiRJ0vJshF3/2m399dfnec97HpMnT+bFL34xJ554IjfeeCPPec5zAFh77bX59re/zW233caRRx7JmDFjGDduHF/+8pcBeOtb38ree+/NxIkTufDCCx8z9sLOjcZ///d/c8QRR7Dddtvx6KOPsummm3LOOecs+U23kdu0S0AyraCv02FIi8x/hUvSyme4LcHVWW3dpj3Jk5N8N8nvkvQn+XmSLdsY79D5epJ0JA1NMinJvy3kXCU5fFDbF5Ic3Lw/NckfkkxPclOSY5ZR2JIkSZKWEwtNsJIE+DHQW1WbV1U38J/ARssiuA6YBAybYDX+Crw7yeoLOH9kVU0FpgIHJdm0rdFJkiRJWq6NVMHaDXikqk6e11BV11TVRWk5McmMJNcleS3Mr0D9OslZSX6f5IQkByS5oum3edPv1CQnJ+lLckuSfYdOnmStJKc0116d5GVN+8FJfpLk/CQzk7wzyXubPpcleWLTb/Mkv2gqbxcl2XrQ3Ccl+W0T4/7NlCcAuzRVqPcM83ncDfwSOGiEz2188+eDI/STJEmSHsfHeJYfi/pdjLTJxWSgfwHnXkmrUrM9sAFwZZLfNOe2B7YB/gb8HvhaVe2c5N3A4cARTb9JwM7A5sCFSZ4xZI4PAr+qqjcmWQ+4IskFg2LbgVYycxvwgaraIclngAOBzwL/C7y9qm5N8izgS8DuzfUbA88HtgbOBn4AHAW8v6oel+wN8nHg/5KcMsy5E5McDTwDOKmq/rqQcbQc6e6GPh/BkiRJy4Hx48dz7733sv7669NaUKZOqSruvfdexo8fP3LnxpLsIvh84DtVNRe4K8mvgZ2AfwBXVtVfAJL8DjivueY6WlWxeb5XVY8Ctyb5Pa1kZ7C9gP2SvL85Hg88rXl/YVUNAANJ7gd+OmiO7ZKsDTwX+P6g/2H+a3N9+Ekz9w1JRr3ksap+n+Ryhl9KeGRV/aCZ+5dJnltVvx3t2JIkSdImm2zCHXfcwd13393pUEQr4d1kk01G3X+kBOt6YP8R+gznoUHvHx10/OiQOYfW24YeB3hVVd38mMZWNWqkOcYA9zXPRI0U46L+p4GP0ap4/Xq4k1X1QJJeWkmoCZYkSZJGbdy4cWy6qY/yr6hGegbrV8AaSd46ryHJdkl2AS4CXptkbJINgV2BKxZx/lcnGdM8l7UZcPOQ8+cChzebbZBkh9EOXFX/AP6Q5NXNtUmy/QiXDQBdoxj7JuAG4KXDnU+yGvAs4HejjVed1d9/J8lxj3tJkiRJi2KhCVa1nuh6BbBns0379cD/AP+P1u6C1wLX0ErE/qOq/t8izn87raTs/2g9KzV7yPnjgXHAtc3cxy/i+AcAb0pyDa1q3MtG6H8tMDfJNQvY5GKwjwJDa4UnJpnejHMd8KNFjFeSJEnSCqxjPzSc5FTgnKr6QUcCkAZJJha87XHtVf6cmSRJkh5vQT80vCSbXEgrje7uifT1mUxJkiRpyXQswaqqgzs1tyRJkiQtDSNtciFJkiRJGiUTLEmSJElqExMsCeCufvhUWi9JkiRpMZlgSZIkSVKbmGBJkiRJUpu4TbsEsFE3vK+v01FIkiRpBWcFS5IkSZLaxARLkiRJktrEBEuSJEmS2sRnsCSgf2CA9PYu8nXV09P2WCRJkrTisoIlSZIkSW1igiVJkiRJbeISQQno7uqiz+V+kiRJWkJWsCRJkiSpTUywJEmSJKlNXCIoAQP9A/Smt9NhSNKo9VRPp0OQJA3DCpYkSZIktYkJliRJkiS1iQmWJEmSJLWJz2BJQFd3Fz19PZ0OQ5IkSSs4K1iSJEmS1CYmWJIkSZLUJi4RlID+fkg6HYUktUdVpyOQpFXXiBWsJHOTTE9yTZKrkjx3WQS2gFh6kpzTvD84yRea929PcuAw/Y9N8s8kTxrU9sCg98vNvUmSJEla8Y2mgjWrqqYCJHkR8D/AC0YzeJIAqapHFzvCUaiqkxdy+h7gfcAHhjm32PcmSZIkSUMt6jNY6wB/n3eQ5MgkVya5NslxTdukJDcn+SYwA9glyY1Jvprk+iTnJZnQ9J2a5LLm+h8neULT3ptkWvN+gyQzFxZUU6l6/wJOnwK8NskTF+XeJEmSJGlRjaaCNSHJdGA8sDGwO0CSvYAtgJ2BAGcn2RW4vWk/qKouSzKpOX59Vb0lyfeAVwHfBr4JHF5Vv07yYeAY4Ij23R4AD9BKst7djD/ivWnV090NfX2djkKSJEkrutFUsGZV1dSq2hrYG/hms/Rvr+Z1NXAVsDWtRArgj1V12aAx/lBV05v3/cCkJOsC61XVr5v204Bdl+huFuwk4KAkXUPaF3RvkiRJkrTIFmkXwaq6NMkGwIa0qlb/U1VfGdynqVg9OOTShwa9nwtMGGGqOfwr+Ru/KDEOp6ruS3IG8I6F9Bl8b39d0jklSZIkrXoWKcFKsjUwFrgXOBc4PsnpVfVAkqcAj4x2rKq6P8nfk+xSVRcB/w7Mq2bNBLqBK4D9FyXGhfg0cCULuOch96ZVTH//nTSPEapDqoau4JUkSVrxLMozWNCqWh1UVXOB85JsA1zarKp7AHgDrQrVaB0EnJxkTeD3wCFN+yeB7yV5K/CzRRhvgarqniQ/Bt4zqHlB9yZJkiRJiyzlrxFKJBML3tbpMFZpVrAkSdKKJEl/VU0b2r5ISwSllVV390T6+vwLviRJkpbMov4OliRJkiRpAUywJEmSJKlNTLAkSZIkqU1MsCSAu/rhU2m9JEmSpMVkgiVJkiRJbWKCJUmSJElt4jbtEsBG3fC+vk5HIUmSpBWcFSxJkiRJahMTLEmSJElqE5cISkD/wADp7R11/+rpWWqxSJIkacVlBUuSJEmS2sQES5IkSZLaxARLkiRJktrEZ7AkoLuriz6fq5IkSdISsoIlSZIkSW1igiVJkiRJbeISQQkY6B+gN72dDmOp6KmeTocgSZK0yrCCJUmSJEltYoIlSZIkSW1igiVJkiRJbeIzWBLQ1d1FT19Pp8OQJEnSCs4KliRJkiS1iQmWJEmSJLWJSwQloL8fkk5H0RlVnY5AkiRp5TFiBStJJfn2oOPVktyd5JwRrlsvyWGDjiclmdG8n5rkJYPO7ZfkqMW7hdFJ8vYkB47Q59gk719A+z+TPGlQ2wOD3s9NMj3JNUmuSvLc9kYvSZIkaUUwmiWCDwKTk0xojl8I/HkU160HHLaAc1OB+QlWVZ1dVSeMYszFVlUnV9U3l2CIe4D3LeDcrKqaWlXbA/8J/M8SzCNJkiRpBTXaZ7B+DuzTvH898J15J4ZWfZLMSDIJOAHYvKnsnDjo/OrAh4HXNudem+TgJF9ozp+a5KQkv03y+yT7N+1JcmIz/nVJXtu09yT5dZKzmv4nJDkgyRVNv82HxpnkLUmubCpOP0yy5ig+g1OamJ84Qr91gL+PYjwtR7q7W0vlVsWXJEmS2me0CdZ3gdclGQ9sB1w+imuOAn7XVHaOnNdYVQ8DHwLObM6dOcy1GwPPB/allagBvJJW5Wt7YE/gxCQbN+e2B94ObAP8O7BlVe0MfA04fJjxf1RVOzUVpxuBN43ifh6glWS9e5hzE5pk8aZmzuNHMZ4kSZKklcyoEqyquhaYRKt69fOlGVDjJ1X1aFXdAGzUtD0f+E5Vza2qu4BfAzs1566sqr9U1UPA74DzmvbrmriHmpzkoiTXAQcA244yrpOAg5J0DWmft0Rwa2Bv4JvJqrplgiRJkrTqWpRt2s8GPsmg5YGNOUPGGb+kQQEPDXo/mkRlcP9HBx0/yvA7JZ4KvLOqpgDHMcqYq+o+4AzgHQvpcymwAbDhaMaUJEmStPJYlG3aTwHuq6rrkvQMap9JaykfSXYENm3aB4ChlR5GcW5BLgLeluQ04InArsCRwNaLOA7N3H9JMo5WBWs0m3bM82ngShbw2SXZGhgL3LsYcalD+vvvJDlumcxVdcwymUeSJEnL3qgrWFV1R1WdNMypHwJPTHI98E7glqb/vcAlzaYUJw655kLgmfM2uRhlCD8GrgWuAX4F/EdV/b/Rxj/Ef9N6juwS4KZFubCq7mliWWNQ87xnsKYDZwIHVdXcxYxNkiRJ0goq5TZiEsnEgrctk7msYEmSJK34kvRX1bSh7YuyRFBaaXV3T6Svz8RHkiRJS2ZRNrmQJEmSJC2ECZYkSZIktYkJliRJkiS1iQmWBHBXf6cjkCRJ0krABEuSJEmS2sQES5IkSZLaxARLAtiou9MRSJIkaSVggiVJkiRJbWKCJUmSJEltslqnA5CWB/0DA6S3t9NhLHeqp6fTIUiSJK1QrGBJkiRJUpuYYEmSJElSm5hgSZIkSVKb+AyWBHR3ddHn80aSJElaQlawJEmSJKlNTLAkSZIkqU1cIigBA/0D9Ka302FIy4We6ul0CJIkrbCsYEmSJElSm5hgSZIkSVKbmGBJkiRJUpv4DJYEdHV30dPX0+kwJEmStIKzgiVJkiRJbWKCJUmSJElt4hJBCejvh6TTUUhaHlR1OgJJ0opsxApWkrlJpg96HdW09yaZtqgTJnl5kmcOOv5wkj0X4fpTk/w5yRrN8QZJZo5wzXpJDlvUWNslyaQkM5r3U5O8pFOxSJIkSVp6RrNEcFZVTR30OmEJ53w5MD/BqqoPVdUFizjGXOCNi9B/PaBjCdYQUwETLEmSJGkl1JZnsJJ8OUlfkuuTHDeo/YQkNyS5NsknkzwX2A84samGbd5UpPZv+u+U5LdJrklyRZKuBUz5WeA9SR63xDHJkUmubOacF8sJwObNnCcO6T8pyU1JTk9yY5IfJFmzOded5NdJ+pOcm2Tjpr03ycebGG9JssugsS5KclXzeu6QuVYHPgy8tonltUluTbJhc35MktvmHWvZ6e5uLQvy5cuXL0mSlsRonsGakGT6oOP/qaozh/T5YFX9LclY4JdJtgP+DLwC2LqqKsl6VXVfkrOBc6rqBwBpHnxpko8zgddW1ZVJ1gFmLSCm24GLgX8HfjqvMclewBbAzkCAs5PsChwFTK6qqQsYbyvgTVV1SZJTgMOSfA74PPCyqro7yWuBj/KvytlqVbVzs9zvGGBP4K/AC6tqdpItgO8A85dRVtXDST4ETKuqdzYxbw0cQCtp3BO4pqruXkCckiRJkpZjo0mwZi0kMZnnNUne2oy3Ma0lgDcAs4GvJzkHOGeEMbYC/lJVVwJU1T9G6P8/wFnAzwa17dW8rm6O16aVcN0+wlh/qqpLmvffBt4F/AKYDJzfJIFjgb8MuuZHzZ/9wKTm/TjgC0mm0lrGuOUI8wKc0tzHZ2klb98YxTWSJEmSlkNLvItgkk2B9wM7VdXfk5wKjK+qOUl2BvYA9gfeCey+GON/A9gBuLOq5j+7VFW3NpW11wzuTqvC9pUhY0waYZqhi0KqGev6qnrOAq55qPlzLv/6HN8D3AVsT2v55ewR5qWq/pTkriS706q8HTDSNZIkSZKWT+3Ypn0d4EHg/iQbAS8GepOsDaxZVT9Pcgnw+6b/ADDcs1U3Axsn2alZIthFq3p2yELm/iiPrWCdCxyf5PSqeiDJU4BHFjLnPE9L8pyquhT4N1rLD28GNpzXnmQcsGVVXb+QcdYF7qiqR5McRKvqNdRwsXyNVuXsW1U1dyHjaynp77+TQY8PSvNVHdPpECRJ0gpkNJtcTBiyTftjdhGsqmtoLcm7CTgDmLfUrgs4J8m1tBKW9zbt3wWOTHJ1ks0HjfMw8Frg80muAc4Hxi8ssCbZuWrQ8XlNDJcmuQ74AdBVVfcClySZMXSTi8bNwDuS3Ag8AfhyE8/+wMebeKYDzx3m2sG+BBzU9N+aVuI51IXAM+dtctG0nU1rOaPLAyVJkqQVWGoV3zKpWT54TlVN7mAM04DPVNUunYphVZdMLHhbp8PQcsgKliRJGk6S/qp63O8Ct2OJoJZAWj/cfCg+e9VR3d0T6evzL9KSJElaMm35HawVWVXN7GT1qqpOqKqnV9XFnYpBkiRJUnus8gmWJEmSJLWLCZYEcFd/pyOQJEnSSsAES5IkSZLaxARLkiRJktrEBEuSJEmS2sQESwLYqLvTEUiSJGklYIIlSZIkSW1igiVJkiRJbbJapwOQlgf9AwOkt7fTYSyx6unpdAiSJEmrNCtYkiRJktQmJliSJEmS1CYmWJIkSZLUJj6DJQHdXV30+fySJEmSlpAVLEmSJElqExMsSZIkSWoTlwhKwED/AL3p7XQYkkahp3o6HYIkSQtkBUuSJEmS2sQES5IkSZLaxCWCEtDV3UVPX0+nw5AkSdIKzgqWJEmSJLWJCZYkSZIktYkJliRJkiS1ic9gSUB/PySdjkKSWqo6HYEkaXGNWMFK8sEk1ye5Nsn0JM9q2ldL8rEktzbt05N8cNB1c5u265Nck+R9SYadL8mWSX7ejHVVku8l2ShJT5Jz2nWzSb6W5JnN+1cnuTHJhUmmJTmpXfNIkiRJWjUttIKV5DnAvsCOVfVQkg2A1ZvTHwGeDEypqtlJuoD3Dbp8VlVNbcZ5EnAGsA5wzJA5xgM/A95bVT9t2nqADZfozoZRVW8edPgm4C1VdXFz3DfacZKsVlVz2hqcJEmSpBXeSEsENwbuqaqHAKrqHoAkawJvASZV1ezm3ABw7HCDVNVfk7wVuDLJsVWPWfzwb8Cl85Krpn9vM0/PvLYkOwOfA8YDs4BDqurmJNsC36CV+I0BXgXcCXwP2AQYCxxfVWcm6QXeD7wEeD7w9SRn00rw3l9V+yZZC/g8MBkYBxxbVWclORh4JbB2M+YLRvjstALp7oa+UafYkiRJ0vBGWiJ4HvDUJLck+VKSeUnFM4Dbm6RqVKrq97QSkycNOTUZ6B/FEDcBu1TVDsCHgI817W8HPtdUy6YBdwB7A3dW1fZVNRn4xZBYPkyrYnVAVR05ZJ4PAr+qqp2B3YATm6QLYEdg/6oyuZIkSZL0OAtNsKrqAaAbeCtwN3BmU8l5jCSHNM9b/SnJU5dKpLAu8P0kM4DPANs27ZcC/5XkA8DTq2oWcB3wwiQfT7JLVd2/CPPsBRyVZDrQS6ti9rTm3PlV9bclvxVJkiRJK6MRN7moqrlV1VtVxwDvpLUE7zbgac1zV1TVN5oK0v20qlSPk2QzYC7w1yGnrqeVxI3keODCpiL1UlqJD1V1BrAfrWWDP0+ye1XdQqvadB3wkSQfGsX480MFXlVVU5vX06rqxubcg4swjiRJkqRVzEibXGwFPFpVtzZNU4E/VtU/k3wd+EKStzWbXIzlXxtgDB1nQ+Bk4AtDnr+C1uYX/5lkn6r6WdN/V2BopWhd4M/N+4MHjb0Z8PuqOinJ04DtktwE/K2qvp3kPuDNjN65wOFJDq+qSrJDVV29CNdrBdTffyfJcZ0OQ6uA1n+rkiRJK6uRNrlYG/h8kvWAObQqV29tzn2QVlVpRpIBWhWk02htMAEwoVlmN6659lvAp4dOUFWzkuwLfDbJZ4FHgGuBdwMbDOr6CeC0JEfT2pRintcA/57kEeD/0Xo2aydaz0492ox36Aj3OdjxwGeBa5tt5f9AaydFSZIkSVqoPL6gJK16kokFb+t0GFoFWMGSJGnlkKS/qqYNbR+pgiWtErq7J9LX5198JUmStGRG3ORCkiRJkjQ6JliSJEmS1CYmWBLAXaP5rWtJkiRp4UywJEmSJKlNTLAkSZIkqU1MsCRJkiSpTUywJICNujsdgSRJklYCJliSJEmS1CYmWJIkSZLUJqt1OgBpedA/MEB6ezsdhqRRqp6eTocgSdKwrGBJkiRJUpuYYEmSJElSm5hgSZIkSVKb+AyWBHR3ddHnMx2SJElaQlawJEmSJKlNTLAkSZIkqU1cIigBA/0D9Ka302FI0hLrqZ5OhyBJqzQrWJIkSZLUJiZYkiRJktQmLhGUgK7uLnr6ejodhiRJklZwVrAkSZIkqU1MsCRJkiSpTUywJEmSJKlNfAZLAvr7Iel0FJK0cqnqdASStOyNWMFK8sAwbW9PcuDSCekx87wxyXVJrk0yI8nLkhyU5DtD+m2Q5O4kayQZl+SEJLcmuSrJpUlePMzYvUmmDWlbP8mFSR5I8oWlfX+SJEmSVi6LVcGqqpPbHchgSQI8FfggsGNV3Z9kbWBD4F7gU0nWrKp/NpfsD/y0qh5KcgKwMTC5Od4IeMEop54N/DcwuXlJkiRJ0qgtVoKV5Fjggar6ZJJe4HJgN2A94E1VdVGSscAJQA+wBvDFqvpKkyidBTwBGAccXVVnJZkEnNuM1Q0cBgwADwBU1QPz3if5NfBS4MwmpNcBH02yJvAWYNOqeqi57i7ge6O5r6p6ELg4yTMW53PRiqu7G/r6Oh2FJEmSVnTt2uRitaraGTgCOKZpexNwf1XtBOwEvCXJprSqRK+oqh1pJWWfaipWAFsAX6qqbYGLgbuAPyT5RpKXDprvO7SSKpJMBLYEfgU8A7i9qv7RpvuSJEmSpFFrV4L1o+bPfmBS834v4MAk02lVpdanlUAF+FiSa4ELgKcAGzXX/LGqLgOoqrnA3rSW/90CfKapnAH8DHheknWA1wA/bPpLkiRJUse0axfBh5o/5w4aM8DhVXXu4I5JDqb1LFV3VT2SZCYwvjn94OC+VVXAFcAVSc4HvgEcW1WzkvwCeAWtStZ7m0tuA56WZB2rWJIkSZKWtaW5Tfu5wKFJftUkUlsCfwbWBf7atO0GPH24i5ulf0+uqquapqnAHwd1+Q6tZ7zWAS4FqKp/Jvk68Lkkb6uqh5NsCPRU1feXwj1qJdHffyfJcZ0OQ1rmqo4ZuZMkSRq10SRYaya5Y9Dxp0c59tdoLRe8qnnG6m7g5cDpwE+TXAf0ATct4PpxwCebRGt2c/3bB50/H/gm8PWm0jXP0cBHgBuSzKZVFfvQAub4WZJHmveXVtWrm4raOsDqSV4O7FVVN4zyniVJkiStwlL+CqBEMrHgbZ0OQ1rmrGBJkrR4kvRX1bSh7UtziaC0wujunkhfn3/RlCRJ0pJp1y6CkiRJkrTKM8GSJEmSpDYxwZIA7urvdASSJElaCZhgSZIkSVKbmGBJkiRJUpuYYEmSJElSm5hgSQAbdXc6AkmSJK0ETLAkSZIkqU1MsCRJkiSpTVbrdADS8qB/YID09nY6jMepnp5OhyBJkqRFYAVLkiRJktrEBEuSJEmS2sQlghLQ3dVFn8vxJEmStISsYEmSJElSm5hgSZIkSVKbmGBJkiRJUpv4DJYEDPQP0JveTochdVRP9XQ6BEmSVnhWsCRJkiSpTUywJEmSJKlNXCIoAV3dXfT09XQ6DEmSJK3grGBJkiRJUpuYYEmSJElSm5hgSZIkSVKb+AyWBPT3Q9LpKCR1UlWnI5AkrQxGrGAl+WCS65Ncm2R6kmc17asl+ViSW5v26Uk+OOi6uU3b9UmuSfK+JMPOl2TLJD9vxroqyfeSbJSkJ8k57brZJF9L8szm/auT3JjkwiTTkpzUrnkkSZIkrZoWWsFK8hxgX2DHqnooyQbA6s3pjwBPBqZU1ewkXcD7Bl0+q6qmNuM8CTgDWAc4Zsgc44GfAe+tqp82bT3Ahkt0Z8OoqjcPOnwT8Jaqurg57hvtOElWq6o5bQ1OkiRJ0gpvpCWCGwP3VNVDAFV1D0CSNYG3AJOqanZzbgA4drhBquqvSd4KXJnk2KrHLMT4N+DSeclV07+3madnXluSnYHPAeOBWcAhVXVzkm2Bb9BK/MYArwLuBL4HbAKMBY6vqjOT9ALvB14CPB/4epKzaSV476+qfZOsBXwemAyMA46tqrOSHAy8Eli7GfMFI3x2WoF0d0PfqFNsSZIkaXgjLRE8D3hqkluSfCnJvKTiGcDtTVI1KlX1e1qJyZOGnJoM9I9iiJuAXapqB+BDwMea9rcDn2uqZdOAO4C9gTuravuqmgz8YkgsH6ZVsTqgqo4cMs8HgV9V1c7AbsCJTdIFsCOwf1WZXEmSJEl6nIUmWFX1ANANvBW4GzizqeQ8RpJDmuet/pTkqUslUlgX+H6SGcBngG2b9kuB/0ryAeDpVTULuA54YZKPJ9mlqu5fhHn2Ao5KMh3opVUxe1pz7vyq+tuS34okSZKkldGIuwhW1VxaiUZvkuuAg2gtv3takq6qGqiqbwDfaJKfscONk2QzYC7w1yGnrmd0y+2OBy6sqlckmdTERFWdkeRyYB/g50neVlW/SrIjraWAH0nyy6ZqNRoBXlVVNw+J/1nAg6McQyuY/v47SY7rdBhazlQdM3InSZKkQRZawUqyVZItBjVNBf5YVf8Evg58odmkgiRj+dcGGEPH2RA4GfjCkOevoLX5xXOT7DOo/65JJg/pty7w5+b9wYP6bgb8vqpOAs4CtksyEfhnVX0bOJHW0r7ROhc4PGlt2p1kh0W4VpIkSdIqbKQK1trA55OsB8wBbqO1XBBazyodD8xIMkBr44nTaG0wATChWWY3rrn2W8Cnh05QVbOS7At8NslngUeAa4F3AxsM6voJ4LQkR9PalGKe1wD/nuQR4P/RejZrJ1rPTj3ajHfoCPc52PHAZ4Frm23l/0BrJ0VJkiRJWqg8vqAkrXqSiQVv63QYWs64RFCSJC1Ikv6qmja0fcRnsKRVQXf3RPr6/Mu0JEmSlsxI27RLkiRJkkbJBEuSJEmS2sQESwK4azS/dS1JkiQtnAmWJEmSJLWJCZYkSZIktYkJliRJkiS1iQmWBLBRd6cjkCRJ0krABEuSJEmS2sQES5IkSZLaZLVOByAtD/oHBkhvb6fDWGlUT0+nQ5AkSeoIK1iSJEmS1CYmWJIkSZLUJi4RlIDuri76XNYmSZKkJWQFS5IkSZLaxARLkiRJktrEBEuSJEmS2sRnsCRgoH+A3vR2OgxJWmw91dPpECRJWMGSJEmSpLYxwZIkSZKkNnGJoAR0dXfR09fT6TAkSZK0grOCJUmSJEltYoIlSZIkSW1igiVJkiRJbeIzWBLQ3w9Jp6OQpJVDVacjkKTOGbGCleSBYdrenuTApRPSY+Z5Y5LrklybZEaSlyU5KMl3hvTbIMndSdZIMi7JCUluTXJVkkuTvHiYsXuTTBvS9sIk/c2c/Ul2X9r3KEmSJGnlsVgVrKo6ud2BDJYkwFOBDwI7VtX9SdYGNgTuBT6VZM2q+mdzyf7AT6vqoSQnABsDk5vjjYAXjHLqe4CXVtWdSSYD5wJPaeOtSZIkSVqJLVaCleRY4IGq+mSSXuByYDdgPeBNVXVRkrHACUAPsAbwxar6SpMonQU8ARgHHF1VZyWZRCuhuRzoBg4DBoAHAKrqgXnvk/waeClwZhPS64CPJlkTeAuwaVU91Fx3F/C90dxXVV096PB6YEKSNeaNpZVXdzf09XU6CkmSJK3o2rXJxWpVtTNwBHBM0/Ym4P6q2gnYCXhLkk2B2cArqmpHWknZp5qKFcAWwJeqalvgYuAu4A9JvpHkpYPm+w6tpIokE4EtgV8BzwBur6p/tOGeXgVcZXIlSZIkabTatcnFj5o/+4FJzfu9gO2S7N8cr0srgboD+FiSXYFHaS3B26jp88equgygquYm2ZtWcrYH8Jkk3VV1LPAz4EtJ1gFeA/yw6d+Wm0myLfDx5h4kSZIkaVTalWDNq/LMHTRmgMOr6tzBHZMcTOtZqu6qeiTJTGB8c/rBwX2rqoArgCuSnA98Azi2qmYl+QXwClqVrPc2l9wGPC3JOotbxUqyCfBj4MCq+t3ijKEVT3//nSTHdToMaZmpOmbkTpIkaZEtzd/BOhc4NMk4gCRbJlmLViXrr01ytRvw9OEuTjIxyY6DmqYCfxx0/B1aidVGwKUAzaYXXwc+l2T1ZpwNk7x6NAEnWY9WdeyoqrpktDcqSZIkSTC6CtaaSe4YdPzpUY79NVrLBa9qnrG6G3g5cDrw0yTXAX3ATQu4fhzwyeYZq9nN9W8fdP584JvA15tK1zxHAx8Bbkgym1ZV7EMLmONnSR5p3l8KXEPrOa4PJZl3zV5V9ddR3bEkSZKkVVrKXwOUSCYWvK3TYUjLjEsEJUlaMkn6q2ra0PZ2PYMlrdC6uyfS1+dfOCVJkrRkluYzWJIkSZK0SjHBkiRJkqQ2cYmgBHBXP3yqPb+jtlje57OQkiRJKwMrWJIkSZLUJiZYkiRJktQmLhGUADbqhvf1dToKSZIkreCsYEmSJElSm5hgSZIkSVKbmGBJkiRJUpv4DJYE9A8MkN7eTochSZKkUaqenk6HMCwrWJIkSZLUJiZYkiRJktQmLhGUgO6uLvqW0zKzJEmSVhxWsCRJkiSpTUywJEmSJKlNTLAkSZIkqU18BksCBvoH6E1vp8OQtALpqZ5OhyBJWg5ZwZIkSZKkNjHBkiRJkqQ2cYmgBHR1d9HT19PpMCRJkrSCs4IlSZIkSW1igiVJkiRJbeISQQno74ek01FIUudUdToCSVo5WMGSJEmSpDYZMcFKMjfJ9CQzkvw0yXrtmDjJwUm+0I6xhoy7S5Lrm5gntHv8Zo7/WhrjSpIkSVqxjaaCNauqplbVZOBvwDuWckxL6gDgf5qYZ43UOcniLJM0wZIkSZL0OIuaXFwKbAeQZGfgc8B4YBZwSFXdnORgYD9gTWBz4MdV9R/NNYcA/wncB1wDPNS0TwJOATYA7m7Guj3Jqc3YOwBPAt4IHAg8B7i8qg4eHFySNwOvAV6U5MXAG4BPAC8GCvhIVZ2ZpAc4Hvg7sHWSbYATgB5gDeCLVfWVJBsDZwLrNJ/VocA+wIQk04Hrq+qARfwMtRzq7oa+vk5HIUmSpBXdqBOsJGOBPYCvN003AbtU1ZwkewIfA17VnJtKKyl6CLg5yeeBOcBxQDdwP3AhcHXT//PAaVV1WpI3AicBL2/OPYFWQrUfcDbwPODNwJVJplbV9HkxVtXXkjwfOKeqfpDkVU0s29NK3q5M8pum+47A5Kr6Q5K3AvdX1U5J1gAuSXIe8Erg3Kr6aHP/a1bVRUneWVVTR/vZSZIkSVo1jCbBmleteQpwI3B+074ucFqSLWhVh8YNuuaXVXU/QJIbgKfTSnB6q+rupv1MYMum/3NoJTMA36JVdZrnp1VVSa4D7qqq65rrrwcmAdMXEvvzge9U1VzgriS/BnYC/gFcUVV/aPrtBWyXZP9B97YFcCVwSpJxwE8GJ3OSJEmSNNRoEqxZVTU1yZrAubSewTqJ1hK7C6vqFc0Sv95B1zw06P3cUc6zIPPGenTIuI8u4bgPDnof4PCqOndopyS70loWeGqST1fVN5dgTi2n+vvvJDmu02FIHVd1TKdDkCRphTbqbdqr6p/Au4D3NRtDrAv8uTl98CiGuBx4QZL1m4rQqwed+y3wuub9AcBFo41rBBcBr00yNsmGwK7AFcP0Oxc4tImLJFsmWSvJ02lVzb4KfI3WskKAR+b1lSRJkqR5FqkCVFVXJ7kWeD2tZXynJTka+Nkorv1LkmNpbZRxH49d2nc48I0kR9JscrEocS3Ej2ktP7yG1jLG/6iq/5dk6yH9vkZrueFVSdLE8HJam14cmeQR4AFaG2wA/C9wbZKr3ORCkiRJ0jwpf7pdIplY8LZOhyF1nEsEJUkanST9VTVtaPuSPMMkrTS6uyfS1+dfLCVJkrRkRv0MliRJkiRp4UywJEmSJKlNXCIoAdzVD59Kp6OQJEnSaL1v+dxLwgqWJEmSJLWJCZYkSZIktYlLBCWAjbrhfX2djkKSJEkrOCtYkiRJktQmJliSJEmS1CYmWJIkSZLUJiZYEtA/MEB6ezsdhiRJklZwJliSJEmS1CYmWJIkSZLUJiZYEtDd1UX19HQ6DEmSJK3gTLAkSZIkqU1MsCRJkiSpTUywJEmSJKlNVut0ANLyYKB/gN70djoMSR3QUz2dDkGStBKxgiVJkiRJbWKCJUmSJElt4hJBCejq7qKnr6fTYUiSJGkFZwVLkiRJktrEBEuSJEmS2sQlghLQ3w9Jp6OQtLKo6nQEkqROsYIlSZIkSW0yYoKVZFKSGUPaepJUkpcOajsnSU/zvjdJ36Bz0xJ/ZEiSJEnSym1JKlh3AB9cyPknJXnxEowvSZIkSSuURXoGK8lmwA+BM4BrgHFJXlhV5w/T/URaCdj/LXGU0lLW3Q19fSP3kyRJkhZm1BWsJFvRSq4OBq5smj8KHL2ASy4FHk6y25IEKEmSJEkritEmWBsCZwEHVNU18xqr6jcASZ6/gOs+woITMEmSJElaqYx2ieD9wO3A84EbhpybV8WaM/SiqvpVko8Az16SIKWlrb//TpLjOh2GVhFVx3Q6BEmStJSMtoL1MPAK4MAk/zb4RFWdBzwB2G4B134E+I/FjlCSJEmSVhCjfgarqh4E9gXeA6wz5PRHgacu4LqfA3cvboCSJEmStKJI+XPzEsnEgrd1OgytIlwiKEnSii9Jf1VNG9q+SNu0Syur7u6J9PX5l15JkiQtmSX5oWFJkiRJ0iAmWJIkSZLUJiZYEsBd/Z2OQJIkSSsBEyxJkiRJahMTLEmSJElqExMsCWCj7k5HIEmSpJWACZYkSZIktYkJliRJkiS1iQmWJEmSJLXJap0OQFoe9A8MkN7eTocxrOrp6XQIkiRJGiUrWJIkSZLUJiZYkiRJktQmLhGUgO6uLvpciidJkqQlZAVLkiRJktrEBEuSJEmS2sQlghIw0D9Ab3o7HYYktUVP9XQ6BElaZVnBkiRJkqQ2McGSJEmSpDYxwZIkSZKkNvEZLAno6u6ip6+n02FIkiRpBWcFS5IkSZLaxARLkiRJktrEJYIS0N8PSaejkKRVS1WnI5Ck9rOCJUmSJEltMmKClWRukulJZiT5fpI12zFxkp8nWa9NY62W5O4kJ7RjvHZK8vYkB3Y6DkmSJElL32gqWLOqampVTQYeBt7ejomr6iVVdV87xgJeCNwCvDppz0KvtCxxha+qTq6qb7YjJkmSJEnLt0VNIC4CnpHkpUkuT3J1kguSbASQ5AVNtWt6c64rycZJfjOoCrZL03dmkg2SnJDkHfMmSHJskvc3749McmWSa5Mct5C4Xg98DrgdeM6gsV6S5KYk/UlOSnJO075hkvOTXJ/ka0n+2MQyKcnNSb4JzACeOlwMSdZK8rMk1zT39Nqm/YQkNzR9Pzn4fpJsneSKQbFNSnJd8747ya+bOM9NsvEifi9aQt3drWcBfPny5cvXsntJ0spo1AlWktWAFwPXARcDz66qHYDvAv/RdHs/8I6qmgrsAswC/g04t2nbHpg+ZOgzgdcMOn4NcGaSvYAtgJ2BqUB3kl2HiWs8sCfwU+A7tJKtee1fAV5cVd3AhoMuOwb4VVVtC/wAeNqgc1sAX2rObbWAGPYG7qyq7ZvK3i+SrA+8Ati2qrYDPjI4zqq6CVg9yaZN02ub+xwHfB7Yv4nzFOCjQ+9TkiRJ0vJvNAnWhCTTgT5aFaKvA5sA5zYVmCOBbZu+lwCfTvIuYL2qmgNcCRyS5FhgSlUNDB68qq4GnpRkYpLtgb9X1Z+AvZrX1cBVwNa0kp2h9gUurKpZwA+BlycZ2/T/fVX9oen3nUHXPJ9WYkhV/QL4+6Bzf6yqy5r3C4rhOuCFST6eZJequh+4H5gNfD3JK4F/DhPr92glVjR/nkkriZsMnN98zkc3n68kSZKkFcxotmmf1VSf5kvyeeDTVXV2kh7gWICqOiHJz4CXAJckeVFV/aap+uwDnJrk08M8k/R9YH/gybSSDoAA/1NVXxky9zuAtzSHL6FVsXp+kplN2/rA7sDdo7i34Tw4eLrhYmji2LGZ/yNJfllVH06yM7BHcy/vbOIY7Ezg+0l+BFRV3ZpkCnB9VT0HdUx//50sfBWqtGqrOqbTIUiStEJY3E0c1gX+3Lw/aF5jks2r6rqq+jitytXWSZ4O3FVVXwW+Buw4zHhnAq+jlZh8v2k7F3hjkrWbsZ+S5ElV9cVm042pwAO0liI+raomVdUk4B20kq6bgc2STGrGm1c5glal7TXNuHsBT1jAfQ4bQ5KJwD+r6tvAicCOTZ91q+rnwHtoLYd8jKr6HTAX+G/+lUjeDGyY5DnNHOOSbDv0WkmSJEnLv8X9oeFjaVVi/g78Cpj3XNERSXYDHgWuB/6PVuJ0ZJJHaCVEj9uyvKquT9IF/Lmq/tK0nZdkG+DSZmPAB4A3AH8ddOkraD1L9dCgtrOATwCHAofRej7qQVoJ3zzHAd9J8u/ApcD/AwaAtYfEtaAYngGcmORR4JFmri7grObZrwDvXcBndyatpGzTZo6Hk+wPnJRkXVrfyWebz0+SJEnSCiS1Em/jk2Ttqnogrezoi8CtVfWZJGsAc6tqTlM5+vLQZZBatUybNq36+vo6HYYkSZJWEEn6q2ra0PbFrWCtKN6S5CBgdVobVcx7luppwPfS+p2rh/nXM12SJEmStNhW6gSrqj4DfGaY9luBHZZ9RJIkSZJWZou7yYUkSZIkaQgTLAngrv5ORyBJkqSVgAmWJEmSJLWJCZYkSZIktYkJlgSwUXenI5AkSdJKwARLkiRJktrEBEuSJEmS2sQES5IkSZLaZKX+oWFptPoHBkhv7yJdUz09SyUWSZIkrbisYEmSJElSm5hgSZIkSVKbuERQArq7uuhzyZ8kSZKWkBUsSZIkSWoTEyxJkiRJahOXCErAQP8AventdBiSVnE91dPpECRJS8gKliRJkiS1iQmWJEmSJLWJCZYkSZIktYnPYElAV3cXPX09nQ5DkiRJKzgrWJIkSZLUJiZYkiRJktQmLhGUgP5+SDodhSQtmqpORyBJGmrEClaSuUmmJ5mR5PtJ1lwWgQ2J4eVJnrms55UkSZKkRTGaJYKzqmpqVU0GHgbePpqBk7SzOvZyYNgEq83zSJIkSdJiW9RnsC4CnpFkrSSnJLkiydVJXgaQ5OAkZyf5FfDLJGsn+UaS65Jcm+RVTb+9klya5KqmKrZ20z4zySea/lckeUaS5wL7ASc2lbTNk/Qm+WySPuDdSfZo4riuiWuNQeMd18xzXZKt2/XBSZIkSdJQo67+NJWiFwO/AD4I/Kqq3phkPeCKJBc0XXcEtquqvyX5OHB/VU1pxnhCkg2Ao4E9q+rBJB8A3gt8uLn+/qqakuRA4LNVtW+Ss4FzquoHzTgAq1fVtCTjgVuBParqliTfBA4FPtuMd09V7ZjkMOD9wJsX/WPSyq67G/r6Oh2FJEmSVnSjqWBNSDId6ANuB74O7AUc1bT3AuOBpzX9z6+qvzXv9wS+OG+gqvo78Gxay/0uaa4/CHj6oPm+M+jP5ywkrjObP7cC/lBVtzTHpwG7Dur3o+bPfmDSwm5UkiRJkpbEaCpYs6pq6uCGtEpIr6qqm4e0Pwt4cITxQisJe/0CztcC3g810jzzPNT8ORd3TZQkSZK0FC1uwnEucHiSw6uqkuxQVVcP0+984B3AEdBaIghcBnwxyTOq6rYkawFPGVSBei1wQvPnpU3bANC1gFhuBibNGw/4d+DXi3lfWkX1999Jclynw9ByquqYTocgSZJWEIv7Q8PHA+OAa5Nc3xwP5yPAE5ot3q8Bdququ4GDge8kuZZWEjV484knNO3vBt7TtH0XOLLZyGLzwRNU1WzgEOD7Sa4DHgVOXsz7kiRJkqTFllqOfqUwyUxgWlXd0+lYtGpJJha8rdNhaDllBUuSJA2VpL+qpg1t95kkCejunkhfn3+JliRJ0pJZrhKsqprU6RgkSZIkaXEt7jNYkiRJkqQhTLAkSZIkqU1MsCRJkiSpTUywJEmSJKlNTLAkSZIkqU1MsCRJkiSpTUywJEmSJKlNTLAkSZIkqU1MsCRJkiSpTVbrdADS8qB/YID09nY6DI1S9fR0OgRJkqRhWcGSJEmSpDYxwZIkSZKkNnGJoAR0d3XR57IzSZIkLSErWJIkSZLUJiZYkiRJktQmLhGUgIH+AXrT2+kwJK2Aeqqn0yFIkpYjVrAkSZIkqU1MsCRJkiSpTUywJEmSJKlNfAZLArq6u+jp6+l0GJIkSVrBWcGSJEmSpDYxwZIkSZKkNnGJoAT090PS6SgkafFVdToCSRKMooKVZG6S6UlmJPlpkvWa9olJfrCAa3qTTGtHgEl2TvKbJDcnuTrJ15KsmeTgJF9oxxzNPD8fdG/vSnJjktOT7JfkqHbNI0mSJGnlNZoK1qyqmgqQ5DTgHcBHq+pOYP+lGBtJNgK+D7yuqi5t2vYHuto9V1W9ZNDhYcCeVXVHc3z2aMdJslpVzWlrcJIkSZJWCIu6RPBSYDuAJJOAc6pqcpIJwDeA7YGbgAnzLkjyJuADwH3ANcBDVfXOJBsCJwNPa7oeUVWXDJnvHcBp85IrgKr6QTPu/E5JXgocDawO3AscUFV3JXkB8Ll5lwK7AmsDZwLrNPd/aFVdlGQmMA34CLAZ8H9JTgH+DkxbWMxJjgU2b667HXj9KD9PLSe6u6Gvr9NRSJIkaUU36gQryVhgD+Drw5w+FPhnVW2TZDvgquaaicB/AzsCA8CvaCVZ0Ep8PlNVFyd5GnAusM2QcScDp40ivIuBZ1dVJXkz8B/A+4D3A+9okqC1gdnAW4Fzq+qjzT2tOXigqnp7kr2B3arqniQHDzq9sJifCTy/qmaNIl5JkiRJK6HRJFgTkkwHngLcCJw/TJ9dgZMAquraJNc27TsDv66qvwEk+T6wZXNuT+CZgypR6yRZu6oeWIz72AQ4M8nGtKpYf2jaLwE+neR04EdVdUeSK4FTkowDflJV0xdhnmFjbt6fbXIlSZIkrdpGs037vGewng6E1rK9ds397Kqa2ryeMkxydT3QPYqxPg98oaqmAG8DxgNU1QnAm2ktWbwkydZV9RtaCeGfgVOTHNimmB9chHEkSZIkrYRGvUSwqv6Z5F3AT5J8acjp3wD/BvwqyWSa57SAK4HPJnkCrSWCrwKua86dBxwOnAiQZOow1aQvAFck+VlVXd70eyWtytRg69JKmAAOmteYZPOqug64LslOwNZJZgF3VNVXk6xBa/niN0f5MYwmZq2A+vvvJDmu02FoFVV1TKdDkCRJbbJIPzRcVVcD1/L4TRy+DKyd5Ebgw0B/0//PwMeAK2glRTOB+5tr3gVMS3JtkhuAtw8z313A64BPNtu03wi8iFayNtixwPeT9AP3DGo/otle/lrgEeD/gB7gmiRXA6/lX5tgjMaIMUuSJEladaWW8i8TznuuKslqwI+BU6rqx0t1UmkRJROrtbpUWvasYEmStOJJ0l9Vj/vt30Xdpn1xHJtkT1rPRZ0H/GQZzCktku7uifT1+ZdcSZIkLZmlnmBV1fuX9hySJEmStDxYpGewJEmSJEkLZoIlSZIkSW1igiVJkiRJbWKCJUmSJEltYoIlSZIkSW1igiVJkiRJbWKCJUmSJEltYoIlSZIkSW1igiVJkiRJbWKCJUmSJEltYoIlSZIkSW1igiVJkiRJbWKCJUmSJEltYoIlSZIkSW1igiVJkiRJbWKCJUmSJEltYoIlSZIkSW1igiVJkiRJbWKCJUmSJEltYoIlSZIkSW1igiVJkiRJbWKCJUmSJEltkqrqdAxSxyUZAG7udBxapjYA7ul0EFqm/M5XPX7nqx6/81VLp7/vp1fVhkMbV+tEJNJy6OaqmtbpILTsJOnzO1+1+J2vevzOVz1+56uW5fX7domgJEmSJLWJCZYkSZIktYkJltTyv50OQMuc3/mqx+981eN3vurxO1+1LJfft5tcSJIkSVKbWMGSJEmSpDYxwZIkSZKkNjHB0iolyd5Jbk5yW5Kjhjm/RpIzm/OXJ5nUgTDVRqP4zt+b5IYk1yb5ZZKndyJOtc9I3/mgfq9KUkmWuy1+NXqj+b6TvKb55/z6JGcs6xjVXqP49/rTklyY5Orm3+0v6UScap8kpyT5a5IZCzifJCc1/5u4NsmOyzrGwUywtMpIMhb4IvBi4JnA65M8c0i3NwF/r6pnAJ8BPr5so1Q7jfI7vxqYVlXbAT8APrFso1Q7jfI7J0kX8G7g8mUbodppNN93ki2A/wSeV1XbAkcs6zjVPqP8Z/xo4HtVtQPwOuBLyzZKLQWnAnsv5PyLgS2a11uBLy+DmBbIBEurkp2B26rq91X1MPBd4GVD+rwMOK15/wNgjyRZhjGqvUb8zqvqwqr6Z3N4GbDJMo5R7TWaf84Bjqf1H1BmL8vg1Haj+b7fAnyxqv4OUFV/XcYxqr1G850XsE7zfl3gzmUYn5aCqvoN8LeFdHkZ8M1quQxYL8nGyya6xzPB0qrkKcCfBh3f0bQN26eq5gD3A+svk+i0NIzmOx/sTcD/LdWItLSN+J03S0eeWlU/W5aBaakYzT/jWwJbJrkkyWVJFvZfwbX8G813fizwhiR3AD8HDl82oamDFvX/75eq1To1sSQtT5K8AZgGvKDTsWjpSTIG+DRwcIdD0bKzGq1lQz20KtS/STKlqu7rZFBaql4PnFpVn0ryHOBbSSZX1aOdDkyrBitYWpX8GXjqoONNmrZh+yRZjdbSgnuXSXRaGkbznZNkT+CDwH5V9dAyik1Lx0jfeRcwGehNMhN4NnC2G12ssEbzz/gdwNlV9UhV/QG4hVbCpRXTaL7zNwHfA6iqS4HxwAbLJDp1yqj+/35ZMcHSquRKYIskmyZZndaDr2cP6XM2cFDzfn/gV+Wvca/IRvzOk+wAfIVWcuWzGSu+hX7nVXV/VW1QVZOqahKt5+72q6q+zoSrJTSaf6//hFb1iiQb0Foy+PtlGKPaazTf+e3AHgBJtqGVYN29TKPUsnY2cGCzm+Czgfur6i+dCsYlglplVNWcJO8EzgXGAqdU1fVJPgz0VdXZwNdpLSW4jdbDlK/rXMRaUqP8zk8E1ga+3+xncntV7dexoLVERvmdayUxyu/7XGCvJDcAc4Ejq8qVCSuoUX7n7wO+muQ9tDa8ONj/WLpiS/IdWv+hZIPm2bpjgHEAVXUyrWftXgLcBvwTOKQzkbbE/71JkiRJUnu4RFCSJEmS2sQES5IkSZLaxARLkiRJktrEBEuSJEmS2sQES5IkSZLaxARLkiRJktrEBEuSJEmS2uT/Ayol86ZapNo6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>F1 micro</th>\n",
       "      <th>F1 macro</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Testing time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>0.255614</td>\n",
       "      <td>0.489493</td>\n",
       "      <td>0.435096</td>\n",
       "      <td>2.887325</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.216997</td>\n",
       "      <td>0.485502</td>\n",
       "      <td>0.465792</td>\n",
       "      <td>0.339291</td>\n",
       "      <td>0.024779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive-Aggressive</th>\n",
       "      <td>0.321057</td>\n",
       "      <td>0.561244</td>\n",
       "      <td>0.493981</td>\n",
       "      <td>1.034138</td>\n",
       "      <td>0.010176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kNN</th>\n",
       "      <td>0.249056</td>\n",
       "      <td>0.443992</td>\n",
       "      <td>0.409919</td>\n",
       "      <td>0.050888</td>\n",
       "      <td>21.194478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random forest</th>\n",
       "      <td>0.311651</td>\n",
       "      <td>0.515086</td>\n",
       "      <td>0.431439</td>\n",
       "      <td>16.669483</td>\n",
       "      <td>1.061377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC L2</th>\n",
       "      <td>0.302643</td>\n",
       "      <td>0.545035</td>\n",
       "      <td>0.490332</td>\n",
       "      <td>2.241432</td>\n",
       "      <td>0.010112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD Classifier</th>\n",
       "      <td>0.214745</td>\n",
       "      <td>0.412230</td>\n",
       "      <td>0.309460</td>\n",
       "      <td>0.991599</td>\n",
       "      <td>0.009709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC L1</th>\n",
       "      <td>0.305359</td>\n",
       "      <td>0.548948</td>\n",
       "      <td>0.493242</td>\n",
       "      <td>6.318007</td>\n",
       "      <td>0.010235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD Classifier</th>\n",
       "      <td>0.247135</td>\n",
       "      <td>0.469633</td>\n",
       "      <td>0.378155</td>\n",
       "      <td>1.325039</td>\n",
       "      <td>0.009919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elastic-Net penalty</th>\n",
       "      <td>0.216467</td>\n",
       "      <td>0.416107</td>\n",
       "      <td>0.313618</td>\n",
       "      <td>1.853612</td>\n",
       "      <td>0.009740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mutltinomial NB</th>\n",
       "      <td>0.041995</td>\n",
       "      <td>0.099698</td>\n",
       "      <td>0.087004</td>\n",
       "      <td>0.298564</td>\n",
       "      <td>0.076589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB</th>\n",
       "      <td>0.154402</td>\n",
       "      <td>0.495161</td>\n",
       "      <td>0.482802</td>\n",
       "      <td>0.363866</td>\n",
       "      <td>0.100028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complement NB</th>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.280517</td>\n",
       "      <td>0.267817</td>\n",
       "      <td>0.298933</td>\n",
       "      <td>0.080510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC L1 feature select</th>\n",
       "      <td>0.272637</td>\n",
       "      <td>0.510527</td>\n",
       "      <td>0.447086</td>\n",
       "      <td>8.053515</td>\n",
       "      <td>0.010053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Score  F1 micro  F1 macro  Training time  \\\n",
       "Ridge Classifier       0.255614  0.489493  0.435096       2.887325   \n",
       "Perceptron             0.216997  0.485502  0.465792       0.339291   \n",
       "Passive-Aggressive     0.321057  0.561244  0.493981       1.034138   \n",
       "kNN                    0.249056  0.443992  0.409919       0.050888   \n",
       "Random forest          0.311651  0.515086  0.431439      16.669483   \n",
       "LinearSVC L2           0.302643  0.545035  0.490332       2.241432   \n",
       "SGD Classifier         0.214745  0.412230  0.309460       0.991599   \n",
       "LinearSVC L1           0.305359  0.548948  0.493242       6.318007   \n",
       "SGD Classifier         0.247135  0.469633  0.378155       1.325039   \n",
       "Elastic-Net penalty    0.216467  0.416107  0.313618       1.853612   \n",
       "Mutltinomial NB        0.041995  0.099698  0.087004       0.298564   \n",
       "Bernoulli NB           0.154402  0.495161  0.482802       0.363866   \n",
       "Complement NB          0.008412  0.280517  0.267817       0.298933   \n",
       "SVC L1 feature select  0.272637  0.510527  0.447086       8.053515   \n",
       "\n",
       "                       Testing time  \n",
       "Ridge Classifier           0.003275  \n",
       "Perceptron                 0.024779  \n",
       "Passive-Aggressive         0.010176  \n",
       "kNN                       21.194478  \n",
       "Random forest              1.061377  \n",
       "LinearSVC L2               0.010112  \n",
       "SGD Classifier             0.009709  \n",
       "LinearSVC L1               0.010235  \n",
       "SGD Classifier             0.009919  \n",
       "Elastic-Net penalty        0.009740  \n",
       "Mutltinomial NB            0.076589  \n",
       "Bernoulli NB               0.100028  \n",
       "Complement NB              0.080510  \n",
       "SVC L1 feature select      0.010053  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score algorithm:\n",
      "Passive-Aggressive: 0.3210571636749023\n",
      "________________________________________________________________________________\n",
      "Best f1 micro algorithm:\n",
      "Passive-Aggressive: 0.5612437277025264\n",
      "________________________________________________________________________________\n",
      "Best f1 macro algorithm:\n",
      "Passive-Aggressive: 0.4939811439619441\n",
      "________________________________________________________________________________\n",
      "Fastest training algorithm:\n",
      "kNN: 0.0508875846862793\n",
      "________________________________________________________________________________\n",
      "Fastest testing algorithm:\n",
      "Ridge Classifier: 0.003274679183959961\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(lem_tokens.map(lambda x: ' '.join(x)), dataset['Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f945b87",
   "metadata": {},
   "source": [
    "Les rÃ©sultats sont quasi identiques avec cette Ã©tape supplÃ©mentaire. Nous laissons donc tomber la lemmatisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541b351",
   "metadata": {},
   "source": [
    "# Recherche des hyperparamÃ¨tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58b02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['Body'], dataset['Tags'], test_size=0.2)\n",
    "mlb = MultiLabelBinarizer().fit(dataset['Tags'].to_list())\n",
    "y_train = mlb.transform(y_train)\n",
    "y_test = mlb.transform(y_test)\n",
    "target_names = mlb.classes_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eedb067",
   "metadata": {},
   "source": [
    "Pour peaufinner notre modÃ¨le, nous allons modifier plusieurs hyperparamÃ¨tres :\n",
    "* Le nombre de features sÃ©lectionnÃ©es par Chi2\n",
    "* La crÃ©ation de n-gram par TF-IDF\n",
    "* La tolÃ©rance de notre LinearSVC\n",
    "* Le paramÃ¨tre de rÃ©gularisation C de LinearSVC\n",
    "\n",
    "Comme nous avons dÃ©jÃ  testÃ© la pÃ©nalitÃ© L2 lors du benchmark, celle-ci ne sera pas testÃ©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd86428d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jbrichardet/Documents/workspace/machine_learning/PStack/.venv-p5/lib/python3.10/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5806963900279893"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "    \n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', get_preprocessing_pipeline(500)),\n",
    "    ('linearSVC', OneVsRestClassifier(LinearSVC(penalty='l1', dual=False,\n",
    "                                           random_state=5, max_iter=5000), n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'linearSVC__estimator__C': np.linspace(98, 101, 4),\n",
    "    'linearSVC__estimator__tol': np.logspace(-4, 0, 5),\n",
    "    'preprocessing__feature-reduction__k': [500, 1000],\n",
    "    'preprocessing__tf-idf__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipe, param_grid, cv=5, scoring=make_scorer(metrics.f1_score, average='macro'), error_score='raise', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bde0a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessing',\n",
      "                 Pipeline(steps=[('tf-idf',\n",
      "                                  TfidfVectorizer(max_df=0.5,\n",
      "                                                  ngram_range=(1, 2),\n",
      "                                                  stop_words='english',\n",
      "                                                  sublinear_tf=True)),\n",
      "                                 ('feature-reduction',\n",
      "                                  SelectKBest(k=1000,\n",
      "                                              score_func=<function chi2 at 0x7f2f96e7ce50>))])),\n",
      "                ('linearSVC',\n",
      "                 OneVsRestClassifier(estimator=LinearSVC(C=98.0, dual=False,\n",
      "                                                         max_iter=5000,\n",
      "                                                         penalty='l1',\n",
      "                                                         random_state=5,\n",
      "                                                         tol=0.01),\n",
      "                                     n_jobs=-1))])\n",
      "{'linearSVC__estimator__C': 98.0, 'linearSVC__estimator__tol': 0.01, 'preprocessing__feature-reduction__k': 1000, 'preprocessing__tf-idf__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc32f297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Opts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m      3\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf-idf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m----> 4\u001b[0m opts \u001b[38;5;241m=\u001b[39m \u001b[43mOpts\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, feature_names, target_names)\n\u001b[1;32m      5\u001b[0m benchmark(data, model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinearSVC L1\u001b[39m\u001b[38;5;124m'\u001b[39m, opts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Opts' is not defined"
     ]
    }
   ],
   "source": [
    "data = (X_train, X_test, y_train, y_test)\n",
    "model = grid_search.best_estimator_\n",
    "feature_names = model.named_steps['preprocessing'].named_steps['tf-idf'].get_feature_names_out()\n",
    "opts = Opts(True, True, feature_names, target_names)\n",
    "benchmark(data, model, 'LinearSVC L1', opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf435e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClusterMixin, TransformerMixin\n",
    "\n",
    "class ClassifierWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "class InverseMLB(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, mlb):\n",
    "        self.mlb = mlb\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.mlb.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e29ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unit-testing',)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "pip = Pipeline([\n",
    "    ('predict', ClassifierWrapper(grid_search.best_estimator_)),\n",
    "    ('get-labels', InverseMLB(mlb))\n",
    "])\n",
    "pip.fit(X_train, y_train)\n",
    "pip.predict(['Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255f893",
   "metadata": {},
   "source": [
    "# Export du meilleur modÃ¨le entraÃ®nÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9b917a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['app/backend/pipeline.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pip, 'app/backend/pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13c90f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "with open('app/backend/pipeline.plk', 'wb') as f:\n",
    "    cloudpickle.dump(pip, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-p5",
   "language": "python",
   "name": ".venv-p5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
